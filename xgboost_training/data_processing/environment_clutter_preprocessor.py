#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯å¢ƒæ‚æ³¢ä¸“é¡¹å­¦ä¹ æ•°æ®é¢„å¤„ç†å™¨
- ä½¿ç”¨ before_track_points.csv ä½œä¸ºç¯å¢ƒæ‚æ³¢æ ·æœ¬ï¼ˆæ ‡ç­¾ä¸º0ï¼‰
- ä½¿ç”¨ found_points.csv ä½œä¸ºä¿¡å·æ ·æœ¬ï¼ˆæ ‡ç­¾ä¸º1ï¼‰
- ä½¿ç”¨ unfound_points.csv ä½œä¸ºæ‚æ³¢æ ·æœ¬ï¼ˆæ ‡ç­¾ä¸º0ï¼‰
- æ”¯æŒè®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®çš„åˆ†ç¦»åŠ è½½
"""

import os
import glob
import pandas as pd
import numpy as np
from tqdm import tqdm

# --- é…ç½® ---
# é¡¹ç›®æ ¹ç›®å½• 
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
TRAIN_DATA_PATH = os.path.join(PROJECT_ROOT, 'train')
TEST_DATA_PATH = os.path.join(PROJECT_ROOT, 'test')

# ç¼“å­˜æ–‡ä»¶è·¯å¾„
CACHE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'cache')
TRAIN_CACHE_FILE = os.path.join(CACHE_DIR, 'environment_clutter_train.parquet')
VAL_CACHE_FILE = os.path.join(CACHE_DIR, 'environment_clutter_validation.parquet')

# ç”¨äºè®­ç»ƒçš„ç‰¹å¾åˆ—
FEATURES_TO_USE = [
    'range_out', 
    'v_out', 
    'azim_out', 
    'elev1', 
    'energy', 
    'energy_dB', 
    'SNR/10'
]

def _load_files_with_progress(path_pattern, description):
    """ä½¿ç”¨tqdmè¿›åº¦æ¡åŠ è½½æ–‡ä»¶"""
    all_files = glob.glob(path_pattern, recursive=True)
    if not all_files:
        print(f"è­¦å‘Š: åœ¨è·¯å¾„ '{path_pattern}' æœªæ‰¾åˆ°æ–‡ä»¶ã€‚")
        return pd.DataFrame()
    
    df_list = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºåŠ è½½è¿›åº¦
    with tqdm(total=len(all_files), desc=description) as pbar:
        for file_path in all_files:
            try:
                df = pd.read_csv(file_path)
                if not df.empty:
                    # æ·»åŠ æ•°æ®æ¥æºä¿¡æ¯
                    folder_name = os.path.basename(os.path.dirname(file_path))
                    df['data_source'] = folder_name
                    df_list.append(df)
            except Exception as e:
                print(f"  - è­¦å‘Š: æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
            pbar.update(1)
            
    if not df_list:
        return pd.DataFrame()
    
    return pd.concat(df_list, ignore_index=True)

def load_environment_clutter_training_data(force_rebuild=False):
    """
    åŠ è½½ç¯å¢ƒæ‚æ³¢è®­ç»ƒæ•°æ®ã€‚
    ç¯å¢ƒæ‚æ³¢æ•°æ®æ¥è‡ª before_track_points.csvï¼ˆæ ‡ç­¾ä¸º0ï¼‰
    """
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs(CACHE_DIR, exist_ok=True)
    
    if os.path.exists(TRAIN_CACHE_FILE) and not force_rebuild:
        print(f"âœ… å‘ç°è®­ç»ƒæ•°æ®ç¼“å­˜ï¼Œæ­£åœ¨ä»ç¼“å­˜å¿«é€ŸåŠ è½½...")
        try:
            df = pd.read_parquet(TRAIN_CACHE_FILE)
            print(f"âš¡ï¸ è®­ç»ƒæ•°æ®ç¼“å­˜åŠ è½½æˆåŠŸï¼å…± {len(df)} æ¡æ•°æ®ã€‚")
            return df
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {e}ã€‚å°†é‡æ–°ä»CSVåŠ è½½ã€‚")

    print("ğŸš€ å¼€å§‹åŠ è½½ç¯å¢ƒæ‚æ³¢è®­ç»ƒæ•°æ®...")
    
    # ä»trainç›®å½•åŠ è½½before_track_points.csvä½œä¸ºç¯å¢ƒæ‚æ³¢
    env_clutter_path = os.path.join(TRAIN_DATA_PATH, '**', 'before_track_points.csv')
    env_clutter_df = _load_files_with_progress(env_clutter_path, "åŠ è½½ç¯å¢ƒæ‚æ³¢æ•°æ®(before_track)")
    
    if env_clutter_df.empty:
        print("âŒ é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•ç¯å¢ƒæ‚æ³¢è®­ç»ƒæ•°æ®ã€‚")
        return pd.DataFrame()

    # ä¸ºç¯å¢ƒæ‚æ³¢æ•°æ®æ·»åŠ æ ‡ç­¾ï¼ˆæ ‡ç­¾ä¸º0ï¼Œè¡¨ç¤ºæ‚æ³¢ï¼‰
    env_clutter_df['label'] = 0
    
    # æ£€æŸ¥ç‰¹å¾åˆ—
    available_features = [f for f in FEATURES_TO_USE if f in env_clutter_df.columns]
    if len(available_features) != len(FEATURES_TO_USE):
        missing = set(FEATURES_TO_USE) - set(available_features)
        print(f"è­¦å‘Š: ç¼ºå°‘ç‰¹å¾åˆ—: {missing}")
    
    # æ•°æ®æ¸…ç†
    cols_to_keep = available_features + ['label', 'data_source']
    env_clutter_df = env_clutter_df[cols_to_keep].dropna()

    print(f"ç¯å¢ƒæ‚æ³¢æ ·æœ¬æ€»æ•°: {len(env_clutter_df)}")
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if not env_clutter_df.empty:
        try:
            env_clutter_df.to_parquet(TRAIN_CACHE_FILE, index=False)
            print("âœ… è®­ç»ƒæ•°æ®ç¼“å­˜åˆ›å»ºæˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            
    return env_clutter_df

def load_validation_data(force_rebuild=False, use_test_data=True):
    """
    åŠ è½½éªŒè¯æ•°æ®ã€‚
    éªŒè¯æ•°æ®æ¥è‡ª found_points.csvï¼ˆä¿¡å·ï¼Œæ ‡ç­¾ä¸º1ï¼‰å’Œ unfound_points.csvï¼ˆæ‚æ³¢ï¼Œæ ‡ç­¾ä¸º0ï¼‰
    """
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs(CACHE_DIR, exist_ok=True)
    
    cache_suffix = "test" if use_test_data else "train"
    val_cache_file = VAL_CACHE_FILE.replace('.parquet', f'_{cache_suffix}.parquet')
    
    if os.path.exists(val_cache_file) and not force_rebuild:
        print(f"âœ… å‘ç°éªŒè¯æ•°æ®ç¼“å­˜ï¼Œæ­£åœ¨å¿«é€ŸåŠ è½½...")
        try:
            df = pd.read_parquet(val_cache_file)
            print(f"âš¡ï¸ éªŒè¯æ•°æ®ç¼“å­˜åŠ è½½æˆåŠŸï¼å…± {len(df)} æ¡æ•°æ®ã€‚")
            return df
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {e}ã€‚å°†é‡æ–°ä»CSVåŠ è½½ã€‚")

    data_path = TEST_DATA_PATH if use_test_data else TRAIN_DATA_PATH
    data_source = "æµ‹è¯•" if use_test_data else "è®­ç»ƒ"
    
    print(f"ğŸš€ å¼€å§‹åŠ è½½{data_source}æ•°æ®ä½œä¸ºéªŒè¯æ•°æ®...")
    
    # åŠ è½½æ­£æ ·æœ¬ï¼ˆä¿¡å·ç‚¹ï¼‰
    found_path = os.path.join(data_path, '**', 'found_points.csv')
    found_df = _load_files_with_progress(found_path, f"åŠ è½½{data_source}æ­£æ ·æœ¬(found)")
    
    # åŠ è½½è´Ÿæ ·æœ¬ï¼ˆæ‚æ³¢ç‚¹ï¼‰
    unfound_path = os.path.join(data_path, '**', 'unfound_points.csv')
    unfound_df = _load_files_with_progress(unfound_path, f"åŠ è½½{data_source}è´Ÿæ ·æœ¬(unfound)")

    if found_df.empty and unfound_df.empty:
        print(f"âŒ é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•{data_source}éªŒè¯æ•°æ®ã€‚")
        return pd.DataFrame()

    # æ·»åŠ æ ‡ç­¾
    found_df['label'] = 1  # ä¿¡å·
    unfound_df['label'] = 0  # æ‚æ³¢
    
    # æ£€æŸ¥ç‰¹å¾åˆ—å¹¶æ¸…ç†æ•°æ®
    for df_name, df in [("æ­£æ ·æœ¬", found_df), ("è´Ÿæ ·æœ¬", unfound_df)]:
        if not df.empty:
            available_features = [f for f in FEATURES_TO_USE if f in df.columns]
            cols_to_keep = available_features + ['label', 'data_source']
            df = df[cols_to_keep].dropna()

    # åˆå¹¶éªŒè¯æ•°æ®
    val_df = pd.concat([found_df, unfound_df], ignore_index=True)
    print(f"éªŒè¯æ•°æ®æ€»æ•°: {len(val_df)} (æ­£æ ·æœ¬: {len(found_df)}, è´Ÿæ ·æœ¬: {len(unfound_df)})")
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if not val_df.empty:
        try:
            val_df.to_parquet(val_cache_file, index=False)
            print("âœ… éªŒè¯æ•°æ®ç¼“å­˜åˆ›å»ºæˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            
    return val_df

if __name__ == '__main__':
    """ç‹¬ç«‹è¿è¡Œæ¨¡å¼"""
    print("="*60)
    print("ğŸ› ï¸  ç¯å¢ƒæ‚æ³¢ä¸“é¡¹å­¦ä¹ æ•°æ®é¢„å¤„ç†å™¨  ğŸ› ï¸")
    print("="*60)
    
    # è·å–æ•°æ®æ¦‚è§ˆ
    train_folders = glob.glob(os.path.join(TRAIN_DATA_PATH, '*'))
    test_folders = glob.glob(os.path.join(TEST_DATA_PATH, '*'))
    print(f"ğŸ“Š æ•°æ®é›†æ¦‚è§ˆ:")
    print(f"  - è®­ç»ƒæ•°æ®æ–‡ä»¶å¤¹æ•°é‡: {len(train_folders)}")
    print(f"  - æµ‹è¯•æ•°æ®æ–‡ä»¶å¤¹æ•°é‡: {len(test_folders)}")
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦å¼ºåˆ¶é‡å»ºç¼“å­˜
    rebuild_response = input("\nâ“ æ˜¯å¦å¼ºåˆ¶é‡å»ºç¼“å­˜ï¼Ÿ(y/n, é»˜è®¤n): ").lower()
    force_rebuild = rebuild_response in ['y', 'yes']
    
    print("\n--- åŠ è½½ç¯å¢ƒæ‚æ³¢è®­ç»ƒæ•°æ® ---")
    train_data = load_environment_clutter_training_data(force_rebuild=force_rebuild)
    
    print("\n--- åŠ è½½æµ‹è¯•éªŒè¯æ•°æ® ---")
    test_val_data = load_validation_data(force_rebuild=force_rebuild, use_test_data=True)
    
    print("\nâœ… ç¯å¢ƒæ‚æ³¢æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    print(f"ç¯å¢ƒæ‚æ³¢è®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
    print(f"æµ‹è¯•éªŒè¯æ•°æ®: {len(test_val_data)} æ¡")