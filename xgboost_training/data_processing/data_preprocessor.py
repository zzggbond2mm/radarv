#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†å™¨
- å¿«é€ŸåŠ è½½åŸå§‹CSVæ•°æ®
- ä½¿ç”¨tqdmæ˜¾ç¤ºåŠ è½½è¿›åº¦
- å°†å¤„ç†åçš„æ•°æ®ç¼“å­˜ä¸ºParquetæ ¼å¼ï¼Œå®ç°ç§’çº§å¤ç”¨
"""

import os
import glob
import pandas as pd
import numpy as np
from tqdm import tqdm

# --- é…ç½® ---
# æ•°æ®å¤„ç†è„šæœ¬çš„è¾“å‡ºç›®å½•
DATA_DIR = os.path.dirname(os.path.abspath(__file__))
TRAIN_DATA_PATH = os.path.join(DATA_DIR, 'train')

# ç¼“å­˜æ–‡ä»¶è·¯å¾„ (ä½¿ç”¨é«˜æ•ˆçš„Parquetæ ¼å¼)
CACHE_FILE_PATH = os.path.join(TRAIN_DATA_PATH, 'cached_train_data.parquet')

# ç”¨äºè®­ç»ƒçš„ç‰¹å¾åˆ— (ä¸ train_classifier.py ä¿æŒä¸€è‡´)
FEATURES_TO_USE = [
    'range_out', 
    'v_out', 
    'azim_out', 
    'elev1', 
    'energy', 
    'energy_dB', 
    'SNR/10'
]

def _load_files_with_progress(path_pattern, description):
    """ä½¿ç”¨tqdmè¿›åº¦æ¡åŠ è½½æ–‡ä»¶"""
    all_files = glob.glob(path_pattern, recursive=True)
    if not all_files:
        print(f"è­¦å‘Š: åœ¨è·¯å¾„ '{path_pattern}' æœªæ‰¾åˆ°æ–‡ä»¶ã€‚")
        return pd.DataFrame()
    
    df_list = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºåŠ è½½è¿›åº¦
    with tqdm(total=len(all_files), desc=description) as pbar:
        for file_path in all_files:
            try:
                df = pd.read_csv(file_path)
                df_list.append(df)
            except Exception as e:
                print(f"  - è­¦å‘Š: æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
            pbar.update(1)
            
    if not df_list:
        return pd.DataFrame()
    
    return pd.concat(df_list, ignore_index=True)

def _validate_and_clean_data(df, data_type):
    """éªŒè¯å’Œæ¸…ç†æ•°æ®ï¼Œå¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼ (å†…éƒ¨å‡½æ•°)"""
    if df.empty:
        return df
    
    print(f"  - {data_type}åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # æ£€æŸ¥å¿…éœ€çš„ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
    available_features = [f for f in FEATURES_TO_USE if f in df.columns]
    if len(available_features) != len(FEATURES_TO_USE):
        missing = set(FEATURES_TO_USE) - set(available_features)
        print(f"  - è­¦å‘Š: ç¼ºå°‘ç‰¹å¾åˆ—: {missing}")
        print(f"  - å°†ä»…ä½¿ç”¨å¯ç”¨ç‰¹å¾: {available_features}")
    
    # åªä¿ç•™éœ€è¦çš„ç‰¹å¾åˆ—
    df_cleaned = df[available_features].copy()
    
    # å¤„ç†ç¼ºå¤±å€¼
    initial_rows = len(df_cleaned)
    df_cleaned.dropna(inplace=True)
    dropped_rows = initial_rows - len(df_cleaned)
    if dropped_rows > 0:
        print(f"  - åˆ é™¤äº† {dropped_rows} è¡ŒåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ— ç©·å¤§å€¼
    if np.isinf(df_cleaned.select_dtypes(include=[np.number])).any().any():
        print(f"  - è­¦å‘Š: {data_type}åŒ…å«æ— ç©·å¤§å€¼ï¼Œå°†è¿›è¡Œå¤„ç†")
        df_cleaned.replace([np.inf, -np.inf], np.nan, inplace=True)
        df_cleaned.dropna(inplace=True)
    
    print(f"  - {data_type}æ¸…ç†åæ•°æ®å½¢çŠ¶: {df_cleaned.shape}")
    return df_cleaned

def load_and_cache_data(force_rebuild=False):
    """
    åŠ è½½å¹¶ç¼“å­˜è®­ç»ƒæ•°æ®ã€‚
    å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œåˆ™ç›´æ¥ä»ç¼“å­˜åŠ è½½ã€‚å¦åˆ™ï¼Œä»CSVåŠ è½½ï¼Œå¤„ç†ååˆ›å»ºç¼“å­˜ã€‚
    
    :param force_rebuild: å¦‚æœä¸ºTrueï¼Œåˆ™å¼ºåˆ¶ä»CSVé‡æ–°åŠ è½½å¹¶è¦†ç›–ç¼“å­˜ã€‚
    :return: åŒ…å«æ‰€æœ‰æ•°æ®çš„DataFrameã€‚
    """
    # ç¡®ä¿'train'ç›®å½•å­˜åœ¨
    os.makedirs(TRAIN_DATA_PATH, exist_ok=True)
    
    if os.path.exists(CACHE_FILE_PATH) and not force_rebuild:
        print(f"âœ… å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨ä» '{CACHE_FILE_PATH}' å¿«é€ŸåŠ è½½...")
        try:
            df = pd.read_parquet(CACHE_FILE_PATH)
            print(f"âš¡ï¸ ç¼“å­˜åŠ è½½æˆåŠŸï¼å…± {len(df)} æ¡æ•°æ®ã€‚")
            return df
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {e}ã€‚å°†é‡æ–°ä»CSVåŠ è½½ã€‚")

    print("ğŸš€ æœªå‘ç°ç¼“å­˜æˆ–éœ€è¦å¼ºåˆ¶é‡å»ºï¼Œå¼€å§‹ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®...")
    
    # 1. åŠ è½½æ­£æ ·æœ¬
    positive_path = os.path.join(TRAIN_DATA_PATH, '**', 'found_points.csv')
    positive_df = _load_files_with_progress(positive_path, "åŠ è½½æ­£æ ·æœ¬(found)")
    
    # 2. åŠ è½½è´Ÿæ ·æœ¬
    negative_path = os.path.join(TRAIN_DATA_PATH, '**', 'unfound_points.csv')
    negative_df = _load_files_with_progress(negative_path, "åŠ è½½è´Ÿæ ·æœ¬(unfound)")

    if positive_df.empty and negative_df.empty:
        print("âŒ é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•è®­ç»ƒæ•°æ®ã€‚è¯·æ£€æŸ¥ 'train' ç›®å½•ä¸‹çš„CSVæ–‡ä»¶ã€‚")
        return pd.DataFrame()

    # 3. æ•°æ®æ¸…ç†
    print("\n--- å¼€å§‹æ•°æ®éªŒè¯å’Œæ¸…ç† ---")
    positive_df = _validate_and_clean_data(positive_df, "æ­£æ ·æœ¬")
    negative_df = _validate_and_clean_data(negative_df, "è´Ÿæ ·æœ¬")

    # 4. æ·»åŠ æ ‡ç­¾å¹¶åˆå¹¶
    positive_df['label'] = 1
    negative_df['label'] = 0
    
    final_df = pd.concat([positive_df, negative_df], ignore_index=True)
    print(f"\n--- æ•°æ®åˆå¹¶å®Œæˆ ---")
    print(f"æ€»æ•°æ®é‡: {len(final_df)} (æ­£: {len(positive_df)}, è´Ÿ: {len(negative_df)})")
    
    # 5. ä¿å­˜åˆ°ç¼“å­˜
    if not final_df.empty:
        print(f"\nğŸ’¾ æ­£åœ¨åˆ›å»ºç¼“å­˜æ–‡ä»¶: '{CACHE_FILE_PATH}'...")
        try:
            final_df.to_parquet(CACHE_FILE_PATH, index=False)
            print("âœ… ç¼“å­˜åˆ›å»ºæˆåŠŸï¼ä¸‹æ¬¡å°†å®ç°ç§’çº§åŠ è½½ã€‚")
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: å¯èƒ½éœ€è¦å®‰è£… 'pyarrow' åº“ (pip install pyarrow)")
            
    return final_df

if __name__ == '__main__':
    """
    å½“ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ—¶ï¼Œæ‰§è¡Œæ•°æ®åŠ è½½å’Œç¼“å­˜åˆ›å»ºè¿‡ç¨‹ã€‚
    """
    print("="*60)
    print("ğŸ› ï¸  æ•°æ®é¢„å¤„ç†å™¨ç‹¬ç«‹è¿è¡Œæ¨¡å¼  ğŸ› ï¸")
    print("="*60)
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦å¼ºåˆ¶é‡å»ºç¼“å­˜
    rebuild_response = input("â“ æ˜¯å¦å¼ºåˆ¶é‡å»ºç¼“å­˜ï¼Ÿ(ä¼šè¦†ç›–ç°æœ‰ç¼“å­˜, y/n, é»˜è®¤n): ").lower()
    force_rebuild = rebuild_response in ['y', 'yes']
    
    if force_rebuild:
        print("âš¡ï¸ å·²é€‰æ‹©å¼ºåˆ¶é‡å»ºç¼“å­˜ã€‚")
    
    # æ‰§è¡ŒåŠ è½½å’Œç¼“å­˜
    load_and_cache_data(force_rebuild=force_rebuild)
    
    print("\nâœ… é¢„å¤„ç†å®Œæˆã€‚") 