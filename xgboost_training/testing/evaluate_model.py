#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
- åŠ è½½å·²ä¿å­˜çš„XGBoostæ¨¡å‹ã€‚
- ä½¿ç”¨ç‹¬ç«‹çš„ 'test' æ•°æ®é›†è¿›è¡Œæœ€ç»ˆæ€§èƒ½è¯„ä¼°ã€‚
- è¾“å‡ºè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Šå’Œæ··æ·†çŸ©é˜µï¼Œæä¾›æœ€å¯é çš„æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ã€‚
"""

import os
import joblib
import pandas as pd
import numpy as np
import glob
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix

# --- é…ç½® ---
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
DATA_DIR = os.path.dirname(os.path.abspath(__file__))

# å¾…è¯„ä¼°æ¨¡å‹çš„è·¯å¾„
MODEL_PATH = os.path.join(DATA_DIR, 'point_classifier.joblib')
FEATURE_INFO_PATH = os.path.join(DATA_DIR, 'feature_info.joblib')

# ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®é›†è·¯å¾„
TEST_DATA_PATH = os.path.join(DATA_DIR, 'test')

# æ–°å¢ï¼šæµ‹è¯•æ•°æ®ç¼“å­˜è·¯å¾„
CACHE_FILE_PATH = os.path.join(TEST_DATA_PATH, 'cached_test_data.parquet')

# æ–°å¢ï¼šå¿«é€Ÿè¯„ä¼°é€‰é¡¹ï¼ˆé‡‡æ ·æ•°æ®ä»¥åŠ é€Ÿè¯„ä¼°ï¼‰
FAST_EVAL_SAMPLE_SIZE = 50000  # å¿«é€Ÿè¯„ä¼°æ—¶çš„æœ€å¤§æ ·æœ¬æ•°
FAST_EVAL_RATIO = 5  # è´Ÿæ ·æœ¬ä¸æ­£æ ·æœ¬çš„æ¯”ä¾‹ï¼ˆç”¨äºå¿«é€Ÿè¯„ä¼°ï¼‰

# ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„ç‰¹å¾åˆ—
# æˆ‘ä»¬å°†ä» feature_info.joblib åŠ¨æ€åŠ è½½
FEATURES_TO_USE = None 


def _load_files_with_progress(path_pattern, description):
    """ä½¿ç”¨tqdmè¿›åº¦æ¡åŠ è½½æ–‡ä»¶ (ä¸é¢„å¤„ç†å™¨ä¸€è‡´)"""
    all_files = glob.glob(path_pattern, recursive=True)
    if not all_files:
        print(f"è­¦å‘Š: åœ¨è·¯å¾„ '{path_pattern}' æœªæ‰¾åˆ°æ–‡ä»¶ã€‚")
        return pd.DataFrame()
    
    df_list = []
    with tqdm(total=len(all_files), desc=description) as pbar:
        for file_path in all_files:
            try:
                df = pd.read_csv(file_path)
                df_list.append(df)
            except Exception as e:
                print(f"  - è­¦å‘Š: æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
            pbar.update(1)
            
    if not df_list:
        return pd.DataFrame()
    
    return pd.concat(df_list, ignore_index=True)


def load_test_data(force_rebuild=False, fast_eval=False):
    """ä» 'test' æ–‡ä»¶å¤¹åŠ è½½ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®ï¼Œå¹¶ä½¿ç”¨ç¼“å­˜ã€‚"""
    # ç¡®ä¿ 'test' ç›®å½•å­˜åœ¨
    os.makedirs(TEST_DATA_PATH, exist_ok=True)

    # å¿«é€Ÿè¯„ä¼°æ¨¡å¼çš„ç¼“å­˜æ–‡ä»¶
    cache_file = CACHE_FILE_PATH
    if fast_eval:
        cache_file = cache_file.replace('.parquet', '_fast.parquet')

    if os.path.exists(cache_file) and not force_rebuild:
        print(f"âœ… å‘ç°æµ‹è¯•æ•°æ®ç¼“å­˜ï¼Œæ­£åœ¨ä» '{os.path.basename(cache_file)}' å¿«é€ŸåŠ è½½...")
        try:
            df = pd.read_parquet(cache_file)
            mode_str = "å¿«é€Ÿè¯„ä¼°" if fast_eval else "å®Œæ•´"
            print(f"âš¡ï¸ æµ‹è¯•æ•°æ®ç¼“å­˜åŠ è½½æˆåŠŸï¼æ€»å…± {len(df)} æ¡æ•°æ® ({mode_str}æ¨¡å¼)ã€‚")
            return df
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {e}ã€‚å°†é‡æ–°ä»CSVåŠ è½½ã€‚")
    
    print(f"\n--- æ­£åœ¨ä»ç‹¬ç«‹çš„ '{TEST_DATA_PATH}' ç›®å½•åŠ è½½åŸå§‹CSVæ–‡ä»¶ ---")
    
    # 1. åŠ è½½æ­£æ ·æœ¬
    positive_path = os.path.join(TEST_DATA_PATH, '**', 'found_points.csv')
    positive_df = _load_files_with_progress(positive_path, "åŠ è½½æµ‹è¯•æ­£æ ·æœ¬(found)")
    
    # 2. åŠ è½½è´Ÿæ ·æœ¬
    negative_path = os.path.join(TEST_DATA_PATH, '**', 'unfound_points.csv')
    negative_df = _load_files_with_progress(negative_path, "åŠ è½½æµ‹è¯•è´Ÿæ ·æœ¬(unfound)")

    if positive_df.empty and negative_df.empty:
        print(f"âŒ é”™è¯¯: æœªèƒ½åœ¨ '{TEST_DATA_PATH}' ç›®å½•ä¸­åŠ è½½ä»»ä½•æµ‹è¯•æ•°æ®ã€‚")
        return None

    # 3. æ·»åŠ æ ‡ç­¾
    positive_df['label'] = 1
    negative_df['label'] = 0
    
    # 4. å¿«é€Ÿè¯„ä¼°æ¨¡å¼ï¼šé‡‡æ ·æ•°æ®
    if fast_eval and not positive_df.empty:
        print(f"\nğŸš€ å¿«é€Ÿè¯„ä¼°æ¨¡å¼ï¼šå¯¹æ•°æ®è¿›è¡Œé‡‡æ ·ä»¥æé«˜é€Ÿåº¦...")
        
        # è®¡ç®—é‡‡æ ·å¤§å°
        pos_sample_size = min(len(positive_df), FAST_EVAL_SAMPLE_SIZE // (FAST_EVAL_RATIO + 1))
        neg_sample_size = min(len(negative_df), pos_sample_size * FAST_EVAL_RATIO)
        
        print(f"   åŸå§‹æ•°æ®: æ­£æ ·æœ¬ {len(positive_df)}, è´Ÿæ ·æœ¬ {len(negative_df)}")
        print(f"   é‡‡æ ·å: æ­£æ ·æœ¬ {pos_sample_size}, è´Ÿæ ·æœ¬ {neg_sample_size}")
        
        if pos_sample_size > 0:
            positive_df = positive_df.sample(n=pos_sample_size, random_state=42)
        if neg_sample_size > 0:
            negative_df = negative_df.sample(n=neg_sample_size, random_state=42)
    
    # 5. åˆå¹¶æ•°æ®
    test_df = pd.concat([positive_df, negative_df], ignore_index=True)
    
    print(f"\næµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»å…± {len(test_df)} æ¡ (æ­£: {len(positive_df)}, è´Ÿ: {len(negative_df)})")

    # 6. ä¿å­˜åˆ°ç¼“å­˜
    if not test_df.empty:
        print(f"\nğŸ’¾ æ­£åœ¨åˆ›å»ºæµ‹è¯•æ•°æ®ç¼“å­˜æ–‡ä»¶: '{os.path.basename(cache_file)}'...")
        try:
            test_df.to_parquet(cache_file, index=False)
            print("âœ… æµ‹è¯•æ•°æ®ç¼“å­˜åˆ›å»ºæˆåŠŸï¼ä¸‹æ¬¡å°†å®ç°ç§’çº§åŠ è½½ã€‚")
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: å¯èƒ½éœ€è¦å®‰è£… 'pyarrow' åº“ (pip install pyarrow)")
        
    return test_df


def evaluate(force_rebuild_cache=False, fast_eval=False):
    """æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹è¯„ä¼°æµç¨‹ã€‚"""
    global FEATURES_TO_USE
    
    print("="*60)
    mode_str = "å¿«é€Ÿè¯„ä¼°" if fast_eval else "å®Œæ•´è¯„ä¼°"
    print(f"ğŸ› ï¸  XGBoost æ¨¡å‹æœ€ç»ˆæ€§èƒ½è¯„ä¼° (ä½¿ç”¨ç‹¬ç«‹æµ‹è¯•é›† - {mode_str})  ğŸ› ï¸")
    print("="*60)

    # 1. æ£€æŸ¥æ¨¡å‹å’Œç‰¹å¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°äº '{MODEL_PATH}'")
        print("è¯·å…ˆè¿è¡Œ 'train_classifier.py' æ¥è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹ã€‚")
        return
    if not os.path.exists(FEATURE_INFO_PATH):
        print(f"âŒ é”™è¯¯: ç‰¹å¾ä¿¡æ¯æ–‡ä»¶æœªæ‰¾åˆ°äº '{FEATURE_INFO_PATH}'")
        print("è¯·ç¡®ä¿ 'feature_info.joblib' ä¸æ¨¡å‹ä¿å­˜åœ¨åŒä¸€ç›®å½•ã€‚")
        return
        
    # 2. åŠ è½½æ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
    print(f"--- æ­£åœ¨åŠ è½½æ¨¡å‹: {os.path.basename(MODEL_PATH)} ---")
    try:
        model = joblib.load(MODEL_PATH)
        feature_info = joblib.load(FEATURE_INFO_PATH)
        FEATURES_TO_USE = feature_info['features']
        
        print("âœ… æ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯åŠ è½½æˆåŠŸã€‚")
        print(f"   æ¨¡å‹å°†ä½¿ç”¨ä»¥ä¸‹ {len(FEATURES_TO_USE)} ä¸ªç‰¹å¾è¿›è¡Œè¯„ä¼°:")
        print(f"   {FEATURES_TO_USE}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æˆ–ç‰¹å¾æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    # 3. åŠ è½½æµ‹è¯•æ•°æ®
    test_df_raw = load_test_data(force_rebuild=force_rebuild_cache, fast_eval=fast_eval)
    if test_df_raw is None or test_df_raw.empty:
        print("âŒ é”™è¯¯: æœªèƒ½åŠ è½½æµ‹è¯•æ•°æ®ï¼Œè¯„ä¼°ä¸­æ­¢ã€‚")
        return
    
    # 4. æ•°æ®æ¸…ç† (åœ¨åŠ è½½æ¨¡å‹å¹¶è·å–ç‰¹å¾åˆ—è¡¨åè¿›è¡Œ)
    print("\n--- æ¸…ç†æµ‹è¯•æ•°æ® ---")
    initial_rows = len(test_df_raw)
    # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
    test_df = test_df_raw.dropna(subset=FEATURES_TO_USE).copy()
    dropped_rows = initial_rows - len(test_df)
    if dropped_rows > 0:
        print(f"  - æ¸…ç†: åˆ é™¤äº† {dropped_rows} è¡ŒåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®ï¼Œå‰©ä½™ {len(test_df)} æ¡ã€‚")
    else:
        print("  - æ•°æ®å®Œæ•´ï¼Œæ— éœ€æ¸…ç†ã€‚")

    # 5. å‡†å¤‡è¯„ä¼°æ•°æ®
    if test_df.empty:
        print("âŒ é”™è¯¯: æ¸…ç†åæ— å‰©ä½™æ•°æ®å¯ä¾›è¯„ä¼°ã€‚")
        return
        
    X_test = test_df[FEATURES_TO_USE]
    y_test = test_df['label']

    print("\n--- å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹å’Œè¯„ä¼° ---")
    
    # 6. æ‰§è¡Œé¢„æµ‹
    try:
        print("â³ æ­£åœ¨è¿›è¡Œæ¨¡å‹é¢„æµ‹...")
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        print("âœ… é¢„æµ‹å®Œæˆã€‚")
    except Exception as e:
        print(f"âŒ æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: å¯èƒ½æ˜¯æµ‹è¯•æ•°æ®æ ¼å¼ä¸è®­ç»ƒæ•°æ®ä¸åŒ¹é…å¯¼è‡´ã€‚")
        return

    # 7. ç”Ÿæˆå¹¶æ‰“å°è¯„ä¼°æŠ¥å‘Š
    eval_mode = "å¿«é€Ÿè¯„ä¼°" if fast_eval else "å®Œæ•´è¯„ä¼°"
    print(f"\n" + "="*20 + f" æœ€ç»ˆæ€§èƒ½è¯„ä¼°æŠ¥å‘Š ({eval_mode}) " + "="*20)
    
    print(f"\nğŸ“Š åˆ†ç±»æŠ¥å‘Š (ç‹¬ç«‹æµ‹è¯•é›† - {eval_mode}):")
    report = classification_report(y_test, y_pred, target_names=['æ‚æ³¢ (0)', 'ä¿¡å· (1)'])
    print(report)
    
    print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ (ç‹¬ç«‹æµ‹è¯•é›† - {eval_mode}):")
    cm = confusion_matrix(y_test, y_pred)
    # TN, FP
    # FN, TP
    tn, fp, fn, tp = cm.ravel()
    
    print(cm)
    print("\n   --- çŸ©é˜µè§£è¯» ---")
    print(f"   âœ… çœŸæ­£ä¾‹ (TP - True Positives):  {tp:6d}  (æ­£ç¡®è¯†åˆ«çš„ä¿¡å·)")
    print(f"   âŒ å‡æ­£ä¾‹ (FP - False Positives): {fp:6d}  (è¢«è¯¯åˆ¤ä¸ºä¿¡å·çš„æ‚æ³¢)")
    print(f"   âŒ å‡è´Ÿä¾‹ (FN - False Negatives): {fn:6d}  (è¢«æ¼æ‰çš„çœŸå®ä¿¡å·)")
    print(f"   âœ… çœŸè´Ÿä¾‹ (TN - True Negatives):  {tn:6d}  (æ­£ç¡®è¯†åˆ«çš„æ‚æ³¢)")
    print("   ------------------")
    
    # è®¡ç®—å…³é”®æŒ‡æ ‡
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0

    print("\nğŸ“ˆ å…³é”®æŒ‡æ ‡è§£è¯»:")
    print(f"  - å¬å›ç‡ (Recall): {recall:.2%}  (åœ¨æ‰€æœ‰çœŸå®ä¿¡å·ä¸­ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†å…¶ä¸­çš„å¤šå°‘)")
    print(f"  - ç²¾ç¡®ç‡ (Precision): {precision:.2%} (åœ¨æˆ‘ä»¬è®¤ä¸ºæ˜¯ä¿¡å·çš„ç‚¹ä¸­ï¼Œæœ‰å¤šå°‘æ˜¯çœŸä¿¡å·)")
    print(f"  - æ•´ä½“å‡†ç¡®ç‡ (Accuracy): {accuracy:.2%}")
    
    if fast_eval:
        print("\nğŸ’¡ æç¤º: è¿™æ˜¯å¿«é€Ÿè¯„ä¼°ç»“æœã€‚å¦‚éœ€å®Œæ•´è¯„ä¼°ï¼Œè¯·è¿è¡Œ 'python evaluate_model.py --full'")
    
    print("="*70)


def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œè¯„ä¼°"""
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    force_rebuild = '--rebuild' in sys.argv
    fast_eval = '--fast' in sys.argv
    full_eval = '--full' in sys.argv
    
    # é»˜è®¤å¯ç”¨å¿«é€Ÿè¯„ä¼°ï¼Œé™¤éæ˜ç¡®è¦æ±‚å®Œæ•´è¯„ä¼°
    if not full_eval and not fast_eval:
        fast_eval = True
        print("ğŸ’¡ é»˜è®¤ä½¿ç”¨å¿«é€Ÿè¯„ä¼°æ¨¡å¼ã€‚å¦‚éœ€å®Œæ•´è¯„ä¼°ï¼Œè¯·ä½¿ç”¨ --full å‚æ•°ã€‚")
    
    if force_rebuild:
        print("âš¡ï¸ å·²é€‰æ‹©å¼ºåˆ¶é‡å»ºæµ‹è¯•æ•°æ®ç¼“å­˜ã€‚")
    
    if fast_eval:
        print("ğŸš€ ä½¿ç”¨å¿«é€Ÿè¯„ä¼°æ¨¡å¼ (æ•°æ®é‡‡æ ·ä»¥æé«˜é€Ÿåº¦)")
    else:
        print("ğŸ” ä½¿ç”¨å®Œæ•´è¯„ä¼°æ¨¡å¼ (å¤„ç†æ‰€æœ‰æµ‹è¯•æ•°æ®)")
    
    evaluate(force_rebuild_cache=force_rebuild, fast_eval=fast_eval)


if __name__ == '__main__':
    main() 