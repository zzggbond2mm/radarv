#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›å»ºå¯ç§»æ¤çš„æ¨¡å‹å¾®è°ƒå·¥å…·åŒ…

æœ¬è„šæœ¬ç”¨äºåˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ã€è‡ªåŒ…å«çš„æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«åœ¨æ–°ç¯å¢ƒä¸­è¿›è¡Œæ¨¡å‹å¾®è°ƒæ‰€éœ€çš„æ‰€æœ‰æ–‡ä»¶ã€‚
"""
import os
import sys
import shutil
import glob
import pandas as pd
import argparse
from tqdm import tqdm

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„
# æ­£ç¡®åœ°å®šä½åˆ°å·¥ä½œåŒºç›®å½• (radar_visualizer)
WORKSPACE_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(WORKSPACE_ROOT)
TRAINING_SCRIPT_DIR = os.path.join(WORKSPACE_ROOT, 'xgboost_training', 'training')

def load_and_consolidate_positives(source_dir, output_path):
    """åŠ è½½æ‰€æœ‰ä»¥ 'found_points' å¼€å¤´çš„ CSV æ–‡ä»¶å¹¶åˆå¹¶"""
    print("--- æ­¥éª¤ 1: åˆå¹¶æ‰€æœ‰æ­£æ ·æœ¬ (found_points*.csv) ---")
    path_pattern = os.path.join(source_dir, '**', 'found_points*.csv')
    all_files = glob.glob(path_pattern, recursive=True)
    
    if not all_files:
        print(f"âŒ é”™è¯¯: åœ¨ç›®å½• '{source_dir}' ä¸­æœªæ‰¾åˆ°ä»»ä½• 'found_points*.csv' æ–‡ä»¶ã€‚")
        return False

    print(f"å‘ç° {len(all_files)} ä¸ªæ­£æ ·æœ¬æ–‡ä»¶ï¼Œå¼€å§‹åˆå¹¶...")
    df_list = []
    with tqdm(total=len(all_files), desc="åˆå¹¶CSVæ–‡ä»¶") as pbar:
        for file_path in all_files:
            try:
                df = pd.read_csv(file_path)
                if not df.empty:
                    df_list.append(df)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
            pbar.update(1)
    
    if not df_list:
        print("âŒ é”™è¯¯: æœªèƒ½ä»æ–‡ä»¶ä¸­åŠ è½½ä»»ä½•æ•°æ®ã€‚")
        return False
        
    full_df = pd.concat(df_list, ignore_index=True)
    print(f"åˆå¹¶å®Œæˆï¼Œæ€»è®¡ {len(full_df)} æ¡æ­£æ ·æœ¬ã€‚")
    
    print(f"æ­£åœ¨å°†åˆå¹¶åçš„æ•°æ®ä¿å­˜åˆ°: {output_path}")
    full_df.to_csv(output_path, index=False)
    print("âœ… æ­£æ ·æœ¬CSVæ–‡ä»¶å·²ä¿å­˜ã€‚")
    return True

def copy_required_files(package_dir):
    """å¤åˆ¶å¾®è°ƒè„šæœ¬ã€åŸºç¡€æ¨¡å‹å’Œç‰¹å¾æ–‡ä»¶"""
    print("\n--- æ­¥éª¤ 2: å¤åˆ¶å¿…è¦çš„æ–‡ä»¶ ---")
    
    files_to_copy = {
        'fine_tune_model.py': os.path.join(TRAINING_SCRIPT_DIR, 'fine_tune_model.py'),
        'point_classifier.joblib': os.path.join(TRAINING_SCRIPT_DIR, 'point_classifier.joblib'),
        'feature_info.joblib': os.path.join(TRAINING_SCRIPT_DIR, 'feature_info.joblib')
    }
    
    for dest_name, src_path in files_to_copy.items():
        if os.path.exists(src_path):
            print(f"  -> æ­£åœ¨å¤åˆ¶: {dest_name}")
            shutil.copy(src_path, os.path.join(package_dir, dest_name))
        else:
            print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°å¿…è¦æ–‡ä»¶ '{src_path}'ï¼Œå·²è·³è¿‡ã€‚")
    
    print("âœ… æ–‡ä»¶å¤åˆ¶å®Œæˆã€‚")

def create_requirements_file(package_dir):
    """åˆ›å»ºåŒ…å«æ ¸å¿ƒä¾èµ–çš„ requirements.txt æ–‡ä»¶"""
    print("\n--- æ­¥éª¤ 3: åˆ›å»ºç¯å¢ƒä¾èµ–æ–‡ä»¶ (requirements.txt) ---")
    
    # åˆ—å‡ºå·²çŸ¥çš„æ ¸å¿ƒä¾èµ–
    dependencies = [
        "pandas",
        "numpy",
        "xgboost",
        "scikit-learn",
        "joblib",
        "tqdm",
        "pyarrow",
        "gputil"
    ]
    
    req_path = os.path.join(package_dir, 'requirements.txt')
    with open(req_path, 'w') as f:
        f.write("# å¾®è°ƒæ‰€éœ€çš„æ ¸å¿ƒPythonåº“\n")
        f.write("# è¯·åœ¨æ–°ç¯å¢ƒä¸­ä½¿ç”¨ 'pip install -r requirements.txt' å‘½ä»¤å®‰è£…\n\n")
        for dep in dependencies:
            f.write(f"{dep}\n")
            
    print(f"âœ… 'requirements.txt' å·²åˆ›å»ºäº: {req_path}")

def main(args):
    """ä¸»å‡½æ•°"""
    print(f"=== åˆ›å»ºå¾®è°ƒå·¥å…·åŒ…åˆ°ç›®å½•: '{args.output_dir}' ===")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    package_dir = args.output_dir
    os.makedirs(package_dir, exist_ok=True)
    
    # 1. åˆå¹¶æ­£æ ·æœ¬
    consolidated_csv_path = os.path.join(package_dir, 'consolidated_positive_samples.csv')
    if not load_and_consolidate_positives(args.source_train_dir, consolidated_csv_path):
        return
        
    # 2. å¤åˆ¶æ–‡ä»¶
    copy_required_files(package_dir)
    
    # 3. åˆ›å»º requirements.txt
    create_requirements_file(package_dir)
    
    print("\n==================================================")
    print("ğŸ‰ å¾®è°ƒå·¥å…·åŒ…å·²æˆåŠŸåˆ›å»ºï¼")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print(f"1. å°†æ•´ä¸ª '{package_dir}' æ–‡ä»¶å¤¹å¤åˆ¶åˆ°æ‚¨çš„æ–°ç¯å¢ƒã€‚")
    print(f"2. åœ¨æ–°ç¯å¢ƒä¸­ï¼Œæ‰“å¼€ç»ˆç«¯ï¼Œè¿›å…¥ '{package_dir}' æ–‡ä»¶å¤¹ã€‚")
    print("3. (å¯é€‰) è¿è¡Œ 'pip install -r requirements.txt' æ¥å®‰è£…ä¾èµ–ã€‚")
    print("4. è¿è¡Œ 'python fine_tune_model.py'ï¼Œè„šæœ¬å°†è‡ªåŠ¨æŸ¥æ‰¾å¹¶ä½¿ç”¨åŒ…å†…çš„æ–‡ä»¶ã€‚")
    print("==================================================")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å¿…è¦æ–‡ä»¶çš„å¯ç§»æ¤å¾®è°ƒå·¥å…·åŒ…ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    parser.add_argument(
        '--source_train_dir',
        type=str,
        default=os.path.join(WORKSPACE_ROOT, 'train'),
        help='åŒ…å«åŸå§‹ found_points*.csv æ–‡ä»¶çš„è®­ç»ƒæ•°æ®æ ¹ç›®å½•ã€‚\né»˜è®¤: %(default)s'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='finetune_package',
        help='å°†è¦åˆ›å»ºçš„å·¥å…·åŒ…æ–‡ä»¶å¤¹çš„åç§°ã€‚\né»˜è®¤: %(default)s'
    )
    
    args = parser.parse_args()
    main(args) 