#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XGBoost æ¨¡å‹å¾®è°ƒè„šæœ¬

æœ¬è„šæœ¬ç”¨äºå¯¹ä¸€ä¸ªå·²è®­ç»ƒå¥½çš„é€šç”¨åˆ†ç±»æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”æ–°ç¯å¢ƒä¸‹çš„ç‰¹å®šæ‚æ³¢ã€‚
å·¥ä½œæµç¨‹:
1. åŠ è½½ä¸€ä¸ªé€šç”¨çš„åŸºç¡€æ¨¡å‹ (ä¾‹å¦‚ point_classifier.joblib)ã€‚
2. åŠ è½½åŸå§‹è®­ç»ƒé›†ä¸­çš„æ­£æ ·æœ¬ (found_points.csv) ç”¨äºâ€œå¤ä¹ â€ï¼Œé˜²æ­¢æ¨¡å‹é—å¿˜ã€‚
3. åŠ è½½æ–°ç¯å¢ƒä¸­çš„è´Ÿæ ·æœ¬ (before_track_points.csv) ä½œä¸ºéœ€è¦å­¦ä¹ çš„æ–°ç‰¹å¾ã€‚
4. åœ¨åˆå¹¶çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¢é‡è®­ç»ƒ (å¾®è°ƒ)ã€‚
5. è¯„ä¼°å¾®è°ƒåæ¨¡å‹åœ¨æ–°ç¯å¢ƒæ•°æ®ä¸Šçš„è¡¨ç°ã€‚
6. å°†å¾®è°ƒåçš„æ¨¡å‹ä¿å­˜åˆ°æ–°ç¯å¢ƒçš„ç›®å½•ä¸­ã€‚
"""
import os
import sys
import argparse
import pandas as pd
import numpy as np
import glob
import xgboost as xgb
import joblib
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
# from sklearn.model_selection import train_test_split # No longer needed

# No longer needed, using direct fit parameter: from xgboost.callback import EarlyStopping

# --- æ–°å¢ï¼šæ•°æ®ç”Ÿæˆæ‰€éœ€çš„é…ç½® ---
RANGE_GATE = 20  # è·ç¦»é—¨é™
AZIM_GATE = 5    # æ–¹ä½é—¨é™
COLUMN_NAMES = ["outfile_circle_num", "track_flag", "is_longflag", "azim_arg", "elev_arg",
                "azim_pianyi", "elev_pianyi", "target_I", "target_Q", "azim_I", "azim_Q",
                "elev_I", "elev_Q", "datetime", "bowei_index", "range_out", "v_out",
                "azim_out", "elev1", "energy", "energy_dB", "SNR/10", "delta_azi",
                "delta_elev", "high"]

# --- è·¯å¾„é…ç½® ---
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥å…¶ä»–æ¨¡å—
# æ­£ç¡®åœ°å®šä½åˆ°å·¥ä½œåŒºç›®å½• (radar_visualizer)
WORKSPACE_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(WORKSPACE_ROOT)

# ç¼“å­˜æ–‡ä»¶è·¯å¾„
CACHE_DIR = os.path.dirname(os.path.abspath(__file__))
POSITIVE_SAMPLES_CACHE_PATH = os.path.join(CACHE_DIR, 'positive_samples.parquet')


# --- è¾…åŠ©å‡½æ•° ---

def select_directory_dialog():
    """
    æ‰“å¼€ä¸€ä¸ªå›¾å½¢åŒ–å¯¹è¯æ¡†ï¼Œè®©ç”¨æˆ·é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶å¤¹ã€‚
    å¦‚æœç”¨æˆ·å–æ¶ˆé€‰æ‹©ï¼Œåˆ™è¿”å› Noneã€‚
    """
    try:
        import tkinter as tk
        from tkinter import filedialog
    except ImportError:
        print("âš ï¸ Tkinteråº“æœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œæ— æ³•æ‰“å¼€å›¾å½¢åŒ–æ–‡ä»¶å¤¹é€‰æ‹©å™¨ã€‚")
        print("è¯·é€šè¿‡å‘½ä»¤è¡Œ --target_env_dir å‚æ•°æŒ‡å®šè·¯å¾„ã€‚")
        return None

    # åˆ›å»ºä¸€ä¸ªéšè—çš„Tkinteræ ¹çª—å£
    root = tk.Tk()
    root.withdraw()
    
    print("\nè¯·åœ¨å¼¹å‡ºçš„çª—å£ä¸­é€‰æ‹©ç›®æ ‡ç¯å¢ƒæ–‡ä»¶å¤¹...")
    
    # è®¾ç½®å¯¹è¯æ¡†çš„åˆå§‹ç›®å½•ä¸ºé¡¹ç›®çš„'test'æ–‡ä»¶å¤¹
    initial_dir = os.path.join(WORKSPACE_ROOT, 'test')
    if not os.path.isdir(initial_dir):
        initial_dir = WORKSPACE_ROOT

    directory = filedialog.askdirectory(
        title="è¯·é€‰æ‹©åŒ…å«æ–°ç¯å¢ƒæ•°æ®çš„ç›®æ ‡æ–‡ä»¶å¤¹",
        initialdir=initial_dir
    )
    
    if directory:
        print(f"âœ… å·²é€‰æ‹©æ–‡ä»¶å¤¹: {directory}")
        return directory
    else:
        print("âŒ ç”¨æˆ·å–æ¶ˆäº†é€‰æ‹©ã€‚")
        return None

# --- æ–°å¢ï¼šä» make_trackpoint_with_before.py ç§»æ¤çš„æ•°æ®ç”Ÿæˆå‡½æ•° ---
def generate_before_track_points(target_dir):
    """
    åœ¨æŒ‡å®šç›®å½•ä¸­ï¼Œæ ¹æ® matlab å’Œ track æ–‡ä»¶ç”Ÿæˆ 'before_track_points.csv'ã€‚
    è¿”å› True è¡¨ç¤ºæˆåŠŸæˆ–æ— éœ€ç”Ÿæˆï¼Œè¿”å› False è¡¨ç¤ºå¤±è´¥ã€‚
    """
    print(f"  -> å°è¯•åœ¨ '{os.path.basename(target_dir)}' ä¸­ç”Ÿæˆ 'before_track_points.csv'...")
    matlab_path, track_path = None, None
    for file in os.listdir(target_dir):
        if file.endswith("_matlab_front1.txt"):
            matlab_path = os.path.join(target_dir, file)
        elif file.endswith("_track_front1.txt"):
            track_path = os.path.join(target_dir, file)

    if not (matlab_path and track_path):
        print("  -> âŒ æœªæ‰¾åˆ° '_matlab_front1.txt' å’Œ/æˆ– '_track_front1.txt'ï¼Œæ— æ³•ç”Ÿæˆæ•°æ®ã€‚")
        return False

    try:
        mat_txt = pd.read_csv(matlab_path, delimiter='\t', header=0)
        tra_txt = pd.read_csv(track_path, delimiter='\t', header=0)
        mat_da = mat_txt.values
        tra_da = tra_txt.values
    except Exception as e:
        print(f"  -> âŒ è¯»å–æºæ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

    if len(tra_da) == 0:
        print("  -> â„¹ï¸ Track æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•ç¡®å®šè½¨è¿¹å¼€å§‹æ—¶é—´ã€‚")
        return True # Not a failure, just can't generate

    earliest_track_time = np.min(tra_da[:, 1])
    before_track_indices = np.where(mat_da[:, 13] < earliest_track_time)[0]

    if len(before_track_indices) > 0:
        before_track_df = pd.DataFrame(mat_da[before_track_indices], columns=COLUMN_NAMES)
        output_path = os.path.join(target_dir, 'before_track_points.csv')
        before_track_df.to_csv(output_path, index=False)
        print(f"  -> âœ… æˆåŠŸç”Ÿæˆ 'before_track_points.csv' ({len(before_track_df)} æ¡)ã€‚")
    else:
        print("  -> â„¹ï¸ æœªæ‰¾åˆ°è½¨è¿¹å¼€å§‹æ—¶é—´ä¹‹å‰çš„ç‚¹ã€‚")
    
    return True

def generate_found_unfound_points(target_dir):
    """åœ¨æŒ‡å®šç›®å½•ä¸­ï¼Œæ ¹æ® matlab å’Œ track æ–‡ä»¶ç”Ÿæˆ found/unfound_points æ–‡ä»¶ã€‚"""
    print(f"  -> å°è¯•åœ¨ '{os.path.basename(target_dir)}' ä¸­ç”Ÿæˆ found/unfound ç‚¹...")
    matlab_path, track_path = None, None
    for file in os.listdir(target_dir):
        if file.endswith("_matlab_front1.txt"):
            matlab_path = os.path.join(target_dir, file)
        elif file.endswith("_track_front1.txt"):
            track_path = os.path.join(target_dir, file)

    if not (matlab_path and track_path):
        print("  -> âŒ æœªæ‰¾åˆ° '_matlab_front1.txt' å’Œ/æˆ– '_track_front1.txt'ï¼Œæ— æ³•ç”Ÿæˆ found/unfound ç‚¹ã€‚")
        return False
    
    try:
        mat_txt = pd.read_csv(matlab_path, delimiter='\t', header=0)
        tra_txt = pd.read_csv(track_path, delimiter='\t', header=0)
        mat_da = mat_txt.values
        tra_da = tra_txt.values
    except Exception as e:
        print(f"  -> âŒ è¯»å–æºæ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

    len_mat, _ = mat_da.shape
    len_tra, _ = tra_da.shape
    
    found_points = {}  # {mat_index: new_track_id}

    if len_tra > 0:
        unique_track_ids = np.unique(tra_da[:, 0])
        track_id_map = {tid: i for i, tid in enumerate(unique_track_ids, 1)}

        for ii in range(len_tra):
            k1 = np.where(mat_da[:, 13] == tra_da[ii, 1])[0]
            for jj in range(len(k1)):
                idx = k1[jj]
                erro_dis = abs(mat_da[idx, 15] - tra_da[ii, 2])
                if erro_dis < RANGE_GATE:
                    erro_azim = abs(mat_da[idx, 17] - tra_da[ii, 3])
                    if erro_azim < AZIM_GATE:
                        original_track_id = tra_da[ii, 0]
                        new_track_id = track_id_map[original_track_id]
                        found_points[idx] = new_track_id

    found_mat_indices = set(found_points.keys())
    all_mat_indices = set(range(len_mat))
    unfound_mat_indices = all_mat_indices - found_mat_indices
    
    if found_mat_indices:
        found_indices_list = sorted(list(found_mat_indices))
        found_array = mat_da[found_indices_list]
        found_df = pd.DataFrame(found_array, columns=COLUMN_NAMES)
        track_nums = [found_points[i] for i in found_indices_list]
        found_df.insert(0, 'track_num', track_nums)
        output_path = os.path.join(target_dir, 'found_points_trackn.csv')
        found_df.to_csv(output_path, index=False)
        print(f"  -> âœ… æˆåŠŸç”Ÿæˆ 'found_points_trackn.csv' ({len(found_df)} æ¡)ã€‚")
    else:
        print("  -> â„¹ï¸ æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…ç‚¹ã€‚")

    if unfound_mat_indices:
        unfound_array = mat_da[list(unfound_mat_indices)]
        unfound_df = pd.DataFrame(unfound_array, columns=COLUMN_NAMES)
        output_path = os.path.join(target_dir, 'unfound_points_trackn.csv')
        unfound_df.to_csv(output_path, index=False)
        print(f"  -> âœ… æˆåŠŸç”Ÿæˆ 'unfound_points_trackn.csv' ({len(unfound_df)} æ¡)ã€‚")
    else:
         print("  -> â„¹ï¸ æ‰€æœ‰ç‚¹éƒ½å·²åŒ¹é…ï¼Œæ— æœªåŒ¹é…ç‚¹ã€‚")

    return True

# --- æ–°å¢ï¼šæ–‡ä»¶é¢„æ£€æŸ¥å‡½æ•° ---
def ensure_data_files_exist(target_dir):
    """æ£€æŸ¥å¿…è¦çš„æ•°æ®æ–‡ä»¶ï¼Œå¦‚æœç¼ºå°‘åˆ™å°è¯•ç”Ÿæˆã€‚"""
    print("\n--- æ­¥éª¤ 0: æ£€æŸ¥å¹¶å‡†å¤‡æ•°æ®æ–‡ä»¶ ---")

    # æ£€æŸ¥ before_track_points.csv for training
    before_track_exists = os.path.exists(os.path.join(target_dir, 'before_track_points.csv'))
    if not before_track_exists:
        print("ğŸŸ¡ 'before_track_points.csv' ä¸å­˜åœ¨ï¼Œå°è¯•ç”Ÿæˆ...")
        if not generate_before_track_points(target_dir):
            print("\nâŒ 'before_track_points.csv' ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¾®è°ƒã€‚ç¨‹åºé€€å‡ºã€‚")
            return False
    else:
        print("âœ… 'before_track_points.csv' å·²å­˜åœ¨ã€‚")

    # æ£€æŸ¥ found/unfound æ–‡ä»¶ for evaluation
    found_files_exist = glob.glob(os.path.join(target_dir, 'found_points*.csv'))
    unfound_files_exist = glob.glob(os.path.join(target_dir, 'unfound_points*.csv'))
    
    if not found_files_exist or not unfound_files_exist:
        print("ğŸŸ¡ 'found/unfound' è¯„ä¼°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ç”Ÿæˆ...")
        if not generate_found_unfound_points(target_dir):
             print("   -> âš ï¸ 'found/unfound' æ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼Œè¯„ä¼°æ­¥éª¤å°†è¢«è·³è¿‡ã€‚")
    else:
        print("âœ… 'found/unfound' è¯„ä¼°æ–‡ä»¶å·²å­˜åœ¨ã€‚")
    
    return True

def load_positive_samples(positive_data_dir, cache_path, force_recache=False):
    """
    åŠ è½½æ­£æ ·æœ¬ã€‚ä¼˜å…ˆä»æ‰“åŒ…çš„CSVåŠ è½½ï¼Œå…¶æ¬¡æ˜¯ç¼“å­˜ï¼Œæœ€åä»åŸå§‹æ–‡ä»¶åŠ è½½ã€‚
    """
    # æ£€æŸ¥â€œæ‰“åŒ…æ¨¡å¼â€ï¼šæ˜¯å¦å­˜åœ¨ä¸€ä¸ªæœ¬åœ°çš„ã€åˆå¹¶å¥½çš„CSVæ–‡ä»¶
    packaged_csv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'consolidated_positive_samples.csv')
    if os.path.exists(packaged_csv_path):
        print(f"âœ… æ£€æµ‹åˆ°æ‰“åŒ…çš„æ­£æ ·æœ¬æ–‡ä»¶ï¼Œå°†ç›´æ¥ä» '{os.path.basename(packaged_csv_path)}' åŠ è½½ã€‚")
        try:
            return pd.read_csv(packaged_csv_path)
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–æ‰“åŒ…çš„CSVæ–‡ä»¶: {e}")
            return pd.DataFrame()

    # --- ä»¥ä¸‹æ˜¯â€œå¼€å‘æ¨¡å¼â€çš„é€»è¾‘ ---
    print("â„¹ï¸ æœªæ£€æµ‹åˆ°æ‰“åŒ…çš„CSVæ–‡ä»¶ï¼Œåˆ‡æ¢åˆ°å¼€å‘æ¨¡å¼åŠ è½½æ•°æ®...")
    if not force_recache and os.path.exists(cache_path):
        print(f"âœ… ä»ç¼“å­˜åŠ è½½æ­£æ ·æœ¬: {cache_path}")
        try:
            return pd.read_parquet(cache_path)
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–ç¼“å­˜æ–‡ä»¶ {cache_path}: {e}ã€‚å°†é‡æ–°ç”Ÿæˆç¼“å­˜ã€‚")
    
    print("ğŸ”„ ç¼“å­˜æœªæ‰¾åˆ°æˆ–è¢«å¼ºåˆ¶åˆ·æ–°ï¼Œæ­£åœ¨é‡æ–°ç”Ÿæˆæ­£æ ·æœ¬ç¼“å­˜...")
    positive_path_pattern = os.path.join(positive_data_dir, '**', 'found_points*.csv')
    positive_df = load_files_with_progress(positive_path_pattern, "åŠ è½½æ‰€æœ‰ found_points*.csv")
    
    if not positive_df.empty:
        print(f"æ­£åœ¨å°† {len(positive_df)} æ¡æ­£æ ·æœ¬å†™å…¥ç¼“å­˜: {cache_path}")
        try:
            positive_df.to_parquet(cache_path)
            print("âœ… ç¼“å­˜å†™å…¥æˆåŠŸã€‚")
        except Exception as e:
            print(f"âŒ æ— æ³•å†™å…¥ç¼“å­˜æ–‡ä»¶ {cache_path}: {e}")
            print("è¯·ç¡®ä¿å·²å®‰è£… 'pyarrow' åº“: pip install pyarrow")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°æ­£æ ·æœ¬ï¼Œæ— æ³•åˆ›å»ºç¼“å­˜ã€‚")
        
    return positive_df

def load_files_with_progress(path_pattern, description):
    """ä½¿ç”¨tqdmè¿›åº¦æ¡åŠ è½½å¤šä¸ªCSVæ–‡ä»¶"""
    all_files = glob.glob(path_pattern, recursive=True)
    if not all_files:
        print(f"è­¦å‘Š: åœ¨è·¯å¾„ '{path_pattern}' æœªæ‰¾åˆ°æ–‡ä»¶ã€‚")
        return pd.DataFrame()
    
    df_list = []
    with tqdm(total=len(all_files), desc=description) as pbar:
        for file_path in all_files:
            try:
                df = pd.read_csv(file_path)
                if not df.empty:
                    df_list.append(df)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
            pbar.update(1)
            
    if not df_list:
        return pd.DataFrame()
    
    return pd.concat(df_list, ignore_index=True)

def check_gpu_availability():
    """æ£€æŸ¥ç³»ç»Ÿä¸­æ˜¯å¦å­˜åœ¨å¯ç”¨çš„NVIDIA GPU"""
    try:
        import GPUtil
        gpus = GPUtil.getGPUs()
        return len(gpus) > 0
    except (ImportError, Exception):
        return False

def get_finetune_params(use_gpu=False):
    """è·å–é€‚ç”¨äºå¾®è°ƒçš„XGBoostå‚æ•°ã€‚é€šå¸¸ä½¿ç”¨è¾ƒä½çš„å­¦ä¹ ç‡ã€‚"""
    print("è·å–å¾®è°ƒå‚æ•°...")
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'logloss',
        'tree_method': 'hist',
        'n_estimators': 1000,      # å¾®è°ƒé€šå¸¸éœ€è¦è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°
        'max_depth': 8,           # ä¿æŒä¸åŸæ¨¡å‹ç›¸ä¼¼çš„å¤æ‚åº¦
        'learning_rate': 0.05,    # ä½¿ç”¨è¾ƒä½çš„å­¦ä¹ ç‡æ˜¯å¾®è°ƒçš„å…³é”®
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'random_state': 42,
    }
    if use_gpu:
        print("ğŸš€ å·²é…ç½®GPUè¿›è¡Œå¾®è°ƒã€‚")
        params['device'] = 'cuda'
    else:
        print("ğŸ’» å·²é…ç½®CPUè¿›è¡Œå¾®è°ƒã€‚")
    return params

def main(args):
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    # --- è¯Šæ–­æ­¥éª¤ ---
    print("\n--- è¯Šæ–­ä¿¡æ¯ ---")
    try:
        import xgboost
        print(f"âœ… XGBoost ç‰ˆæœ¬: {xgboost.__version__}")
        print(f"   - è·¯å¾„: {xgboost.__file__}")
    except Exception as e:
        print(f"âŒ æ— æ³•è·å– XGBoost ä¿¡æ¯: {e}")
    # print("------------------\n") # No longer needed
    
    print("=== å¼€å§‹æ¨¡å‹å¾®è°ƒæµç¨‹ ===")

    # å¦‚æœæœªé€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šç›®å½•ï¼Œåˆ™æ‰“å¼€é€‰æ‹©å¯¹è¯æ¡†
    if not args.target_env_dir:
        target_dir = select_directory_dialog()
        if not target_dir:
            print("ç¨‹åºå·²é€€å‡ºã€‚")
            sys.exit(0)
        args.target_env_dir = target_dir

    # --- æ–°å¢ï¼šè°ƒç”¨æ–‡ä»¶é¢„æ£€æŸ¥ ---
    ensure_data_files_exist(args.target_env_dir)

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
    print(f"\n--- æ­¥éª¤ 1: åŠ è½½åŸºç¡€æ¨¡å‹ ---")
    if not os.path.exists(args.base_model_path):
        print(f"âŒ é”™è¯¯: åŸºç¡€æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {args.base_model_path}")
        return

    base_model = joblib.load(args.base_model_path)
    print(f"âœ… æˆåŠŸåŠ è½½åŸºç¡€æ¨¡å‹: {args.base_model_path}")

    # ç¡®å®šå¹¶åŠ è½½ç‰¹å¾ä¿¡æ¯æ–‡ä»¶
    feature_info_path = os.path.join(os.path.dirname(args.base_model_path), 'feature_info.joblib')
    if not os.path.exists(feature_info_path):
        print(f"âŒ é”™è¯¯: ç‰¹å¾ä¿¡æ¯æ–‡ä»¶ 'feature_info.joblib' æœªåœ¨æ¨¡å‹ç›®å½•ä¸­æ‰¾åˆ°ã€‚")
        print("æ­¤æ–‡ä»¶å¯¹äºç¡®ä¿ç‰¹å¾ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚")
        return
        
    feature_info = joblib.load(feature_info_path)
    features_to_use = feature_info['features']
    print(f"æ¨¡å‹ä¾èµ– {len(features_to_use)} ä¸ªç‰¹å¾: {features_to_use}")

    # 2. å‡†å¤‡â€œå¤ä¹ â€æ•°æ® (åŸå§‹æ­£æ ·æœ¬)
    print("\n--- æ­¥éª¤ 2: åŠ è½½æ­£æ ·æœ¬(ä¿¡å·)ç”¨äºå¤ä¹  ---")
    positive_df = load_positive_samples(
        args.positive_data_dir,
        POSITIVE_SAMPLES_CACHE_PATH,
        args.force_recache_positives
    )
    
    if positive_df.empty:
        print("âŒ é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•æ­£æ ·æœ¬ ('found_points.csv')ã€‚")
        return
    
    positive_df['label'] = 1
    positive_df = positive_df[features_to_use + ['label']].dropna()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(positive_df)} æ¡æ­£æ ·æœ¬ç”¨äºå¤ä¹ ã€‚")

    # 3. å‡†å¤‡â€œæ–°å­¦ä¹ â€æ•°æ® (æ–°ç¯å¢ƒæ‚æ³¢)
    print(f"\n--- æ­¥éª¤ 3: ä»ç›®æ ‡ç¯å¢ƒ '{args.target_env_dir}' åŠ è½½æ–°æ‚æ³¢ ---")
    new_clutter_path = os.path.join(args.target_env_dir, 'before_track_points.csv')
    if not os.path.exists(new_clutter_path):
        print(f"âŒ é”™è¯¯: åœ¨ç›®æ ‡ç¯å¢ƒä¸­æœªæ‰¾åˆ°æ–°çš„æ‚æ³¢æ–‡ä»¶: {new_clutter_path}")
        return

    new_clutter_df = pd.read_csv(new_clutter_path)
    new_clutter_df['label'] = 0
    new_clutter_df = new_clutter_df[features_to_use + ['label']].dropna()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(new_clutter_df)} æ¡æ–°ç¯å¢ƒæ‚æ³¢æ ·æœ¬ã€‚")

    # 4. åˆ›å»ºå¾®è°ƒæ•°æ®é›†
    print("\n--- æ­¥éª¤ 4: åˆ›å»ºå¾®è°ƒæ•°æ®é›† ---")
    # ä¸ºäº†é˜²æ­¢æ–°æ‚æ³¢æ•°æ®å¯¹æ¨¡å‹å½±å“è¿‡å¤§ï¼Œå¹¶ä¿æŒå¯¹æ­£æ ·æœ¬çš„è®°å¿†ï¼Œè¿›è¡Œæ•°æ®å‡è¡¡
    # ä½¿ç”¨æŒ‡å®šçš„æ¯”ä¾‹æ¥ç¡®å®šæ­£æ ·æœ¬çš„é‡‡æ ·æ•°é‡
    n_positive_samples = min(len(positive_df), int(len(new_clutter_df) * args.positive_ratio))
    
    print(f"æ ¹æ®æ¯”ä¾‹ {args.positive_ratio}:1ï¼Œå°†ä» {len(positive_df)} æ¡å¯ç”¨æ­£æ ·æœ¬ä¸­é‡‡æ · {n_positive_samples} æ¡ç”¨äºå¾®è°ƒã€‚")

    if n_positive_samples == 0 and len(new_clutter_df) > 0:
        print("âš ï¸ è­¦å‘Š: é‡‡æ ·çš„æ­£æ ·æœ¬æ•°é‡ä¸º0ã€‚è¯·æ£€æŸ¥æ¯”ä¾‹æˆ–æ•°æ®ã€‚")
    
    positive_df_sampled = positive_df.sample(n=n_positive_samples, random_state=42)

    fine_tune_df = pd.concat([positive_df_sampled, new_clutter_df], ignore_index=True)
    fine_tune_df = fine_tune_df.sample(frac=1, random_state=42).reset_index(drop=True) # æ‰“ä¹±æ•°æ®
    print(f"å¾®è°ƒæ•°æ®é›†å·²åˆ›å»ºï¼Œæ€»è®¡ {len(fine_tune_df)} æ¡æ ·æœ¬ã€‚")
    print(f"(åŒ…å« {len(positive_df_sampled)} æ¡æ­£æ ·æœ¬ å’Œ {len(new_clutter_df)} æ¡æ–°æ‚æ³¢æ ·æœ¬)")

    # 5. æ‰§è¡Œæ¨¡å‹å¾®è°ƒ
    print("\n--- æ­¥éª¤ 5: å¼€å§‹æ¨¡å‹å¾®è°ƒ ---")
    X_tune = fine_tune_df[features_to_use]
    y_tune = fine_tune_df['label']

    print(f"å¾®è°ƒè®­ç»ƒé›†: {len(X_tune)}æ¡ (ä½¿ç”¨å…¨éƒ¨å¾®è°ƒæ•°æ®è¿›è¡Œè®­ç»ƒ)")

    # è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®GPU
    gpu_available = check_gpu_availability()
    use_gpu = gpu_available and not args.force_cpu

    if use_gpu:
        print("ğŸš€ æ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†è‡ªåŠ¨ä½¿ç”¨GPUè¿›è¡Œå¾®è°ƒã€‚")
    elif args.force_cpu:
        print("ğŸ’» å·²æ ¹æ® --force-cpu å‚æ•°å¼ºåˆ¶ä½¿ç”¨CPUã€‚")
    else:
        print("ğŸ’» æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†ä½¿ç”¨CPUã€‚")

    model_params = get_finetune_params(use_gpu=use_gpu)
    
    fine_tuned_model = xgb.XGBClassifier(**model_params)
    
    print("\nå¾®è°ƒè¿‡ç¨‹æ—¥å¿— (åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°):")
    fine_tuned_model.fit(
        X_tune, y_tune, 
        xgb_model=base_model, 
        eval_set=[(X_tune, y_tune)], # ä½¿ç”¨è®­ç»ƒé›†è‡ªèº«ä½œä¸ºè¯„ä¼°é›†ä»¥æ˜¾ç¤ºè¿›åº¦
        verbose=10 
    )
    print("âœ… æ¨¡å‹å¾®è°ƒå®Œæˆï¼")

    # 6. åœ¨ç›®æ ‡ç¯å¢ƒä¸Šè¯„ä¼°æ¨¡å‹
    print(f"\n--- æ­¥éª¤ 6: åœ¨ '{args.target_env_dir}' ä¸Šè¯„ä¼°å¾®è°ƒåæ¨¡å‹ ---")
    eval_dfs = []
    # åŠ è½½ç›®æ ‡ç¯å¢ƒä¸­çš„ä¿¡å·å’Œæ‚æ³¢ç”¨äºè¯„ä¼°
    # ä½¿ç”¨globæ”¯æŒæ–‡ä»¶åå˜ä½“ï¼Œä¾‹å¦‚ found_points_trackn.csv
    for basename, label in [('found_points', 1), ('unfound_points', 0)]:
        search_pattern = os.path.join(args.target_env_dir, f'{basename}*.csv')
        found_files = glob.glob(search_pattern)

        if found_files:
            eval_path = found_files[0]
            if len(found_files) > 1:
                print(f"âš ï¸  å‘ç°å¤šä¸ªåŒ¹é… '{basename}*.csv' çš„æ–‡ä»¶ï¼Œå°†åªä½¿ç”¨ç¬¬ä¸€ä¸ª: {eval_path}")

            print(f"  -> æ­£åœ¨åŠ è½½è¯„ä¼°æ•°æ®: {os.path.basename(eval_path)}")
            try:
                eval_df = pd.read_csv(eval_path)
                eval_df['label'] = label
                eval_dfs.append(eval_df)
            except Exception as e:
                print(f"  âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")

    if not eval_dfs:
        print("âš ï¸ è­¦å‘Š: ç›®æ ‡ç¯å¢ƒä¸­æœªæ‰¾åˆ° 'found_points*.csv' æˆ– 'unfound_points*.csv' æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
    else:
        full_eval_df = pd.concat(eval_dfs, ignore_index=True)
        full_eval_df.dropna(subset=features_to_use, inplace=True)
        
        X_eval = full_eval_df[features_to_use]
        y_eval = full_eval_df['label']

        y_pred = fine_tuned_model.predict(X_eval)
        print("\nåˆ†ç±»æŠ¥å‘Š (ç›®æ ‡ç¯å¢ƒ):")
        
        try:
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨æ–°ç‰ˆ scikit-learn çš„å­—å…¸è¾“å‡ºåŠŸèƒ½
            report_dict = classification_report(y_eval, y_pred, target_names=['æ‚æ³¢ (0)', 'ä¿¡å· (1)'], output_dict=True, zero_division=0)
            
            # æå–å¹¶çªå‡ºæ˜¾ç¤ºæ•´ä½“å‡†ç¡®ç‡
            accuracy = report_dict.get('accuracy', 0.0)
            print(f"    - æ•´ä½“å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
            
            # æ‰“å°æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
            print("\n" + "-"*50)
            print(f"{'ç±»åˆ«':<10} | {'ç²¾ç¡®ç‡(P)':<10} | {'å¬å›ç‡(R)':<10} | {'F1-Score':<10}")
            print("-"*50)
            for class_name in ['æ‚æ³¢ (0)', 'ä¿¡å· (1)']:
                metrics = report_dict.get(class_name, {})
                if metrics:
                    p = metrics.get('precision', 0.0)
                    r = metrics.get('recall', 0.0)
                    f1 = metrics.get('f1-score', 0.0)
                    print(f"{class_name:<10} | {p:<10.4f} | {r:<10.4f} | {f1:<10.4f}")
            print("-"*50)

        except (TypeError, KeyError):
            # å¦‚æœå¤±è´¥ (é€šå¸¸å› ä¸ºç‰ˆæœ¬è¾ƒæ—§)ï¼Œåˆ™å›é€€åˆ°æ‰‹åŠ¨è®¡ç®—å’Œæ‰“å°
            print("ï¼ˆæ‚¨çš„ scikit-learn ç‰ˆæœ¬è¾ƒæ—§ï¼Œå°†ä½¿ç”¨å›é€€æŠ¥å‘Šæ¨¡å¼ï¼‰")
            from sklearn.metrics import accuracy_score
            
            accuracy = accuracy_score(y_eval, y_pred)
            print(f"    - æ•´ä½“å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}\n")
            
            # æ‰“å°åŸå§‹æŠ¥å‘Š
            report = classification_report(y_eval, y_pred, target_names=['æ‚æ³¢ (0)', 'ä¿¡å· (1)'])
            print(report)

        print("\næ··æ·†çŸ©é˜µ (ç›®æ ‡ç¯å¢ƒ):")
        cm = confusion_matrix(y_eval, y_pred)
        print(cm)
        if cm.shape == (2, 2):
            print(f"TP(çœŸä¿¡å·): {cm[1,1]}, FP(è¯¯åˆ¤ä¸ºä¿¡å·): {cm[0,1]}, FN(æ¼åˆ¤çš„ä¿¡å·): {cm[1,0]}, TN(çœŸæ‚æ³¢): {cm[0,0]}")

    # 7. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
    output_model_path = os.path.join(args.target_env_dir, args.output_model_name)
    print(f"\n--- æ­¥éª¤ 7: ä¿å­˜æ¨¡å‹ä¸ç‰¹å¾ä¿¡æ¯ ---")
    
    if use_gpu:
        print("æ­£åœ¨å°†æ¨¡å‹è½¬æ¢ä¸ºCPUå…¼å®¹æ¨¡å¼...")
        fine_tuned_model.set_params(device='cpu')
    
    joblib.dump(fine_tuned_model, output_model_path)
    print(f"âœ… å¾®è°ƒåçš„æ¨¡å‹å·²æˆåŠŸä¿å­˜åˆ°: {output_model_path}")

    # åˆ›å»ºå¹¶ä¿å­˜æ–°çš„ç‰¹å¾ä¿¡æ¯æ–‡ä»¶
    new_feature_importance = fine_tuned_model.feature_importances_
    fine_tune_info = {
        'features': features_to_use,
        'feature_importance': dict(zip(features_to_use, new_feature_importance)),
        'training_info': {
            'type': 'fine-tuning',
            'base_model': args.base_model_path,
            'positive_data_source': args.positive_data_dir,
            'finetune_env_source': args.target_env_dir,
            'positive_ratio': args.positive_ratio,
            'used_gpu': use_gpu,
            'model_params': model_params
        }
    }
    
    # æ ¹æ®æ¨¡å‹æ–‡ä»¶åç”Ÿæˆç‰¹å¾ä¿¡æ¯æ–‡ä»¶å
    model_path_base, _ = os.path.splitext(output_model_path)
    feature_info_path = f"{model_path_base}_feature_info.joblib"
    
    joblib.dump(fine_tune_info, feature_info_path)
    print(f"âœ… é…å¥—çš„ç‰¹å¾ä¿¡æ¯å·²ä¿å­˜åˆ°: {feature_info_path}")


if __name__ == '__main__':
    # --- å‘½ä»¤è¡Œå‚æ•°é…ç½® ---
    parser = argparse.ArgumentParser(
        description="å¯¹XGBoostæ‚æ³¢åˆ†ç±»å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°ç¯å¢ƒã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„xgboost_training/trainingç›®å½•ä½œä¸ºé»˜è®¤è·¯å¾„
    default_training_dir = os.path.join(WORKSPACE_ROOT, 'xgboost_training', 'training')
    default_train_data_dir = os.path.join(WORKSPACE_ROOT, 'train')

    parser.add_argument(
        '--base_model_path',
        type=str,
        default=os.path.join(default_training_dir, 'point_classifier.joblib'),
        help='åŸºç¡€é€šç”¨æ¨¡å‹çš„è·¯å¾„ã€‚\né»˜è®¤: %(default)s'
    )
    parser.add_argument(
        '--positive_data_dir',
        type=str,
        default=default_train_data_dir,
        help='(å¼€å‘æ¨¡å¼) ç”¨äºå¤ä¹ çš„åŸå§‹æ­£æ ·æœ¬(found_points.csv)æ‰€åœ¨çš„æ ¹ç›®å½•ã€‚\nåœ¨æ‰“åŒ…æ¨¡å¼ä¸‹æ­¤å‚æ•°æ— æ•ˆã€‚\né»˜è®¤: %(default)s'
    )
    parser.add_argument(
        '--target_env_dir',
        type=str,
        default=None, # è®¾ç½®ä¸ºå¯é€‰å‚æ•°
        help='åŒ…å«æ–°ç¯å¢ƒæ•°æ®(before_track_points.csv)çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚\nå¦‚æœæœªæä¾›æ­¤å‚æ•°ï¼Œå°†æ‰“å¼€å›¾å½¢åŒ–çª—å£è®©ç”¨æˆ·é€‰æ‹©ã€‚'
    )
    parser.add_argument(
        '--output_model_name',
        type=str,
        default='finetuned_model.joblib',
        help='å¾®è°ƒåæ¨¡å‹çš„è¾“å‡ºæ–‡ä»¶åã€‚\nå°†ä¿å­˜åœ¨ --target_env_dir ä¸­ã€‚\né»˜è®¤: %(default)s'
    )
    parser.add_argument(
        '--positive-ratio',
        type=float,
        default=2.0,
        help='å¾®è°ƒæ—¶ï¼Œç”¨äºå¤ä¹ çš„æ­£æ ·æœ¬ä¸æ–°ç¯å¢ƒæ‚æ³¢æ ·æœ¬çš„æ¯”ä¾‹ã€‚\nä¾‹å¦‚: 5 è¡¨ç¤ºæ¯1æ¡æ–°æ‚æ³¢ï¼Œå°±ä½¿ç”¨5æ¡æ­£æ ·æœ¬ã€‚\né»˜è®¤: %(default)s'
    )
    parser.add_argument(
        '--force-cpu',
        action='store_true',
        help='å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œå¾®è°ƒï¼Œå³ä½¿æ£€æµ‹åˆ°GPUã€‚'
    )
    parser.add_argument(
        '--force-recache-positives',
        action='store_true',
        help='(å¼€å‘æ¨¡å¼) å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ­£æ ·æœ¬ç¼“å­˜æ–‡ä»¶ï¼Œå³ä½¿ç¼“å­˜å·²å­˜åœ¨ã€‚\nåœ¨æ‰“åŒ…æ¨¡å¼ä¸‹æ­¤å‚æ•°æ— æ•ˆã€‚'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import xgboost
        import sklearn
        import joblib
        # 'from tqdm import tqdm' is already at the top level, 
        # but we check for the library's existence here for user feedback.
        __import__('tqdm') 
        import pyarrow
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·å®‰è£…: pip install xgboost scikit-learn joblib tqdm pandas pyarrow")
        sys.exit(1)

    main(args) 