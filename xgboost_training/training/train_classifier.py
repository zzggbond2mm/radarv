import os
import pandas as pd
import numpy as np
import glob
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import joblib

# å¯¼å…¥æ–°çš„æ•°æ®é¢„å¤„ç†å™¨
from data_preprocessor import load_and_cache_data, FEATURES_TO_USE

# --- é…ç½® ---
# æ•°æ®å’Œæ¨¡å‹è·¯å¾„ç°åœ¨ç”±data_preprocessorç®¡ç†
DATA_DIR = os.path.dirname(os.path.abspath(__file__))

# è´Ÿæ ·æœ¬ç›¸å¯¹äºæ­£æ ·æœ¬çš„é‡‡æ ·æ¯”ä¾‹ (æ¢å¤åˆ°50ï¼Œå› ä¸º300æ•ˆæœä¸ä½³ï¼Œæˆ‘ä»¬å…ˆä¼˜åŒ–æ¨¡å‹)
NEGATIVE_TO_POSITIVE_RATIO = 50

# æ¨¡å‹è¾“å‡ºè·¯å¾„
MODEL_OUTPUT_PATH = os.path.join(DATA_DIR, 'point_classifier.joblib')


def check_gpu_availability():
    """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
    try:
        import GPUtil
        gpus = GPUtil.getGPUs()
        if len(gpus) > 0:
            print(f"âœ… æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPU:")
            for i, gpu in enumerate(gpus):
                print(f"   GPU {i}: {gpu.name} (æ˜¾å­˜: {gpu.memoryTotal}MB)")
            return True
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨GPU")
            return False
    except ImportError:
        print("âš ï¸  GPUtilæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPUçŠ¶æ€")
        return False
    except Exception as e:
        print(f"âš ï¸  GPUæ£€æµ‹å¤±è´¥: {e}")
        return False


def get_xgboost_params(use_gpu=False):
    """è·å–XGBoostå‚æ•°é…ç½®"""
    if use_gpu:
        print("ğŸš€ é…ç½®GPUè®­ç»ƒå‚æ•° (ä½¿ç”¨æŠ—è¿‡æ‹Ÿåˆå‚æ•°)...")
        try:
            # ä½¿ç”¨ä¸€ç»„å¤æ‚åº¦è¾ƒä½çš„å‚æ•°æ¥é˜²æ­¢è¿‡æ‹Ÿåˆ
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
                'device': 'cuda',
                'tree_method': 'hist',
                # --- æŠ—è¿‡æ‹Ÿåˆå‚æ•°è°ƒæ•´ ---
                'n_estimators': 700,  # æ¨èå€¼ï¼šåœ¨æ€§èƒ½å’Œæ•ˆç‡é—´å–å¾—è‰¯å¥½å¹³è¡¡
                'max_depth': 12,      # æ¨èå€¼ï¼šç»è¿‡éªŒè¯çš„æœ€ä½³æ·±åº¦
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                # -------------------------
                'random_state': 42,
            }
            print("âœ… GPUè®­ç»ƒå‚æ•°é…ç½®å®Œæˆ")
            return params
        except Exception as e:
            print(f"âŒ GPUå‚æ•°é…ç½®å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°CPUè®­ç»ƒ...")
            return get_xgboost_params(use_gpu=False)
    else:
        print("ğŸ’» é…ç½®CPUè®­ç»ƒå‚æ•°...")
        # CPUè®­ç»ƒå‚æ•° (ä¹Ÿç›¸åº”æ›´æ–°)
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'tree_method': 'hist',
            'n_estimators': 700,  # æ¨èå€¼ï¼šåœ¨æ€§èƒ½å’Œæ•ˆç‡é—´å–å¾—è‰¯å¥½å¹³è¡¡
            'max_depth': 12,      # æ¨èå€¼ï¼šç»è¿‡éªŒè¯çš„æœ€ä½³æ·±åº¦
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
        }
        print("âœ… CPUè®­ç»ƒå‚æ•°é…ç½®å®Œæˆ")
        return params


def prepare_dataset(force_rebuild=False):
    """
    åŠ è½½æ•°æ®ï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Œå¹¶å‡†å¤‡æœ€ç»ˆçš„è®­ç»ƒæ•°æ®é›†ã€‚
    æ•°æ®åŠ è½½å’Œæ¸…æ´—è¿‡ç¨‹å·²ç§»è‡³ data_preprocessor.pyã€‚
    """
    print("--- ç¬¬1æ­¥: åŠ è½½å’Œç¼“å­˜æ•°æ® (ä½¿ç”¨data_preprocessor) ---")
    
    # ä»é¢„å¤„ç†å™¨åŠ è½½æ•°æ®ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†ç¼“å­˜
    full_df = load_and_cache_data(force_rebuild=force_rebuild)
    
    if full_df is None or full_df.empty:
        print("âŒ é”™è¯¯: æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®ã€‚")
        return None
        
    # æ–°å¢ï¼šæ•°æ®éªŒè¯å’Œä¿®å¤ - ä¸ºç¼ºå°‘ track_id çš„æ­£æ ·æœ¬å¡«å……é»˜è®¤å€¼
    if 'track_id' in full_df.columns:
        # è¯†åˆ«å‡ºé‚£äº› label ä¸º 1 ä½† track_id ä¸ºç©ºçš„å¼‚å¸¸ç‚¹
        anomalous_points_mask = (full_df['label'] == 1) & (full_df['track_id'].isna())
        num_anomalous_points = anomalous_points_mask.sum()
        
        if num_anomalous_points > 0:
            print(f"--- æ•°æ®éªŒè¯ä¸ä¿®å¤ï¼šå‘ç°å¹¶å¤„ç†å¼‚å¸¸æ•°æ® ---")
            print(f"å‘ç° {num_anomalous_points} ä¸ªæ­£æ ·æœ¬(label=1)ç¼ºå°‘ track_idã€‚")
            print("å°†ä¸ºè¿™äº›ç‚¹å¡«å……é»˜è®¤å€¼ï¼Œè€Œä¸æ˜¯ç§»é™¤å®ƒä»¬ã€‚")
            
            # å®šä¹‰è¦å¡«å……çš„åˆ—å’Œå¯¹åº”çš„é»˜è®¤å€¼
            # track_id: -1 è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæœªè¢«è·Ÿè¸ªçš„ç‹¬ç«‹çœŸå®ç‚¹
            # consecutive_hits: 1 è¡¨ç¤ºè¿™æ˜¯å®ƒè‡ªå·±çš„ç¬¬ä¸€æ¬¡ï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€æ¬¡ï¼‰å‘½ä¸­
            # time_since_first_hit_ns: 0 è¡¨ç¤ºè¿™æ˜¯é¦–æ¬¡å‘ç°
            features_to_impute = {
                'track_id': -1,
                'consecutive_hits': 1,
                'time_since_first_hit_ns': 0
            }
            
            for feature, value in features_to_impute.items():
                if feature in full_df.columns:
                    # ä½¿ç”¨ .loc æ¥ç¡®ä¿åœ¨åŸå§‹DataFrameçš„åˆ‡ç‰‡ä¸Šè¿›è¡Œä¿®æ”¹
                    full_df.loc[anomalous_points_mask, feature] = value
            
            print(f"å·²ä¸º {num_anomalous_points} ä¸ªç‚¹å¡«å……äº†é»˜è®¤çš„èˆªè¿¹ä¿¡æ¯ã€‚")
        else:
            print("--- æ•°æ®éªŒè¯ï¼šæœªå‘ç°ç¼ºå°‘ track_id çš„æ­£æ ·æœ¬ã€‚ ---")

    # æ–°å¢ï¼šè¿‡æ»¤æ‰æŒç»­æ—¶é—´è¿‡é•¿çš„èˆªè¿¹ï¼ˆå¯èƒ½æ˜¯é™æ€æ‚æ³¢ï¼‰
    # å®šä¹‰ä¸€ä¸ªèˆªè¿¹åŒ…å«çš„æœ€å¤§ç‚¹æ•°é˜ˆå€¼ï¼Œè¶…è¿‡åˆ™è¢«è®¤ä¸ºæ˜¯é™æ€æ‚æ³¢
    MAX_POINTS_PER_TRACK = 800  # è¿™ä¸ªå€¼å¯ä»¥æ ¹æ®å®é™…åœºæ™¯è°ƒæ•´

    if 'track_id' in full_df.columns:
        track_point_counts = full_df['track_id'].value_counts()
        # æ‰¾å‡ºé‚£äº›ç‚¹æ•°è¶…è¿‡é˜ˆå€¼çš„ track_id
        long_tracks_to_remove = track_point_counts[track_point_counts > MAX_POINTS_PER_TRACK].index
        
        if not long_tracks_to_remove.empty:
            num_before_filtering = len(full_df)
            print(f"--- è¿‡æ»¤é™æ€æ‚æ³¢ ---")
            print(f"å‘ç° {len(long_tracks_to_remove)} ä¸ªé•¿èˆªè¿¹ (ç‚¹æ•° > {MAX_POINTS_PER_TRACK})ã€‚")
            print(f"å°†è¢«ç§»é™¤çš„èˆªè¿¹ID: {long_tracks_to_remove.tolist()}")
            
            # ä» DataFrame ä¸­ç§»é™¤è¿™äº›èˆªè¿¹çš„æ‰€æœ‰ç‚¹
            full_df = full_df[~full_df['track_id'].isin(long_tracks_to_remove)]
            
            num_after_filtering = len(full_df)
            print(f"å·²ç§»é™¤ {num_before_filtering - num_after_filtering} ä¸ªç‚¹ã€‚")
            print(f"è¿‡æ»¤åå‰©ä½™æ•°æ®é‡: {num_after_filtering}ã€‚")
        else:
            print("--- é™æ€æ‚æ³¢æ£€æŸ¥ï¼šæœªå‘ç°éœ€è¦è¿‡æ»¤çš„è¿‡é•¿èˆªè¿¹ã€‚---")
    else:
        print("âš ï¸ è­¦å‘Š: æ•°æ®ä¸­ç¼ºå°‘ 'track_id' åˆ—ï¼Œæ— æ³•æ‰§è¡Œé™æ€æ‚æ³¢è¿‡æ»¤ã€‚")

    # ä»åŠ è½½çš„æ•°æ®ä¸­åˆ†ç¦»æ­£è´Ÿæ ·æœ¬
    positive_df = full_df[full_df['label'] == 1]
    negative_df = full_df[full_df['label'] == 0]
    
    if positive_df.empty:
        print("âŒ é”™è¯¯: æ•°æ®é›†ä¸­æœªæ‰¾åˆ°ä»»ä½•æ­£æ ·æœ¬(label=1)")
        return None
        
    print(f"\n--- ç¬¬2æ­¥: å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ (æ¬ é‡‡æ ·) ---")
    num_positive = len(positive_df)
    
    # å¦‚æœæ²¡æœ‰è´Ÿæ ·æœ¬ï¼Œåªä½¿ç”¨æ­£æ ·æœ¬ï¼ˆè™½ç„¶ä¸ç†æƒ³ï¼Œä½†å¯ä»¥è¿è¡Œï¼‰
    if negative_df.empty:
        print("âš ï¸ è­¦å‘Š: æ•°æ®é›†ä¸­æœªæ‰¾åˆ°è´Ÿæ ·æœ¬(label=0)ã€‚æ¨¡å‹å°†åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šè®­ç»ƒã€‚")
        return positive_df
        
    num_negative_to_sample = int(num_positive * NEGATIVE_TO_POSITIVE_RATIO)
    
    if len(negative_df) < num_negative_to_sample:
        print(f"è­¦å‘Š: è´Ÿæ ·æœ¬æ•°é‡ ({len(negative_df)}) å°‘äºæœŸæœ›é‡‡æ ·æ•° ({num_negative_to_sample})ï¼Œå°†ä½¿ç”¨æ‰€æœ‰è´Ÿæ ·æœ¬ã€‚")
        sampled_negative_df = negative_df
    else:
        sampled_negative_df = negative_df.sample(n=num_negative_to_sample, random_state=42)
    
    print(f"æŒ‰ {NEGATIVE_TO_POSITIVE_RATIO}:1 çš„æ¯”ä¾‹ï¼Œä»è´Ÿæ ·æœ¬ä¸­éšæœºæŠ½å– {len(sampled_negative_df)} ä¸ªã€‚")

    # åˆå¹¶æˆæœ€ç»ˆæ•°æ®é›†
    final_df = pd.concat([positive_df, sampled_negative_df], ignore_index=True)
    final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True) # æ‰“ä¹±æ•°æ®
    
    print(f"æœ€ç»ˆæ•°æ®é›†å¤§å°: {len(final_df)} (æ­£: {num_positive}, è´Ÿ: {len(sampled_negative_df)})")
    
    # æœ€ç»ˆæ•°æ®éªŒè¯
    print(f"ä½¿ç”¨çš„ç‰¹å¾: {FEATURES_TO_USE}")
    
    return final_df


def train_model(df):
    """
    ä½¿ç”¨ç»™å®šçš„DataFrameè®­ç»ƒXGBoostæ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚
    """
    if df is None or df.empty:
        print("æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹ã€‚")
        return

    print("\n--- ç¬¬3æ­¥: è®¾å¤‡é…ç½®å’Œæ¨¡å‹è®­ç»ƒ ---")
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    gpu_available = check_gpu_availability()
    
    # å†³å®šæ˜¯å¦ä½¿ç”¨GPU
    use_gpu = gpu_available
    if use_gpu:
        print("ğŸš€ å°†ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒä»¥æé«˜é€Ÿåº¦")
    else:
        print("ğŸ’» å°†ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
    
    X = df[FEATURES_TO_USE]
    y = df['label']

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    print(f"è®­ç»ƒé›†: {len(X_train)} æ¡, éªŒè¯é›†: {len(X_val)} æ¡")

    # è®¡ç®—å®é™…çš„ç±»åˆ«æ¯”ä¾‹
    neg_count = (y_train == 0).sum()
    pos_count = (y_train == 1).sum()
    actual_ratio = neg_count / pos_count if pos_count > 0 else 1
    scale_pos_weight = actual_ratio  # ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡
    print(f"è®­ç»ƒé›†ä¸­å®é™…è´Ÿæ­£æ¯”ä¾‹: {actual_ratio:.2f}:1")

    # è·å–è®­ç»ƒå‚æ•°
    model_params = get_xgboost_params(use_gpu=use_gpu)
    model_params['scale_pos_weight'] = scale_pos_weight

    print(f"\nå¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
    print(f"è®­ç»ƒå‚æ•°: {model_params}")
    
    try:
        # åˆå§‹åŒ–å¹¶è®­ç»ƒæ¨¡å‹
        model = xgb.XGBClassifier(**model_params)
        
        # è®­ç»ƒæ¨¡å‹ï¼Œæ˜¾ç¤ºè¿›åº¦
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=True  # æ¢å¤æ˜¾ç¤ºæ¯ä¸€è½®çš„æ—¥å¿—
        )
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ GPUè®­ç»ƒå¤±è´¥: {e}")
        if use_gpu:
            print("ğŸ”„ å°è¯•å›é€€åˆ°CPUè®­ç»ƒ...")
            model_params = get_xgboost_params(use_gpu=False)
            model_params['scale_pos_weight'] = scale_pos_weight
            model = xgb.XGBClassifier(**model_params)
            # ç§»é™¤CPUæ¨¡å¼çš„æ—©åœ
            model.fit(
                X_train, y_train, 
                eval_set=[(X_val, y_val)], 
                verbose=True
            )
            print("âœ… CPUè®­ç»ƒå®Œæˆï¼")
        else:
            raise e
    
    print("\n--- ç¬¬4æ­¥: è¯„ä¼°æ¨¡å‹æ€§èƒ½ ---")
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val)[:, 1]
    
    print("åˆ†ç±»æŠ¥å‘Š (éªŒè¯é›†):")
    print(classification_report(y_val, y_pred, target_names=['æ‚æ³¢ (0)', 'ä¿¡å· (1)']))
    
    print("æ··æ·†çŸ©é˜µ (éªŒè¯é›†):")
    # TN, FP
    # FN, TP
    cm = confusion_matrix(y_val, y_pred)
    print(cm)
    print(f"è§£è¯»: TP(çœŸä¿¡å·): {cm[1,1]}, FP(é”™åˆ¤ä¸ºä¿¡å·): {cm[0,1]}, FN(æ¼åˆ¤çš„ä¿¡å·): {cm[1,0]}, TN(çœŸæ‚æ³¢): {cm[0,0]}")
    
    # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§
    print("\nç‰¹å¾é‡è¦æ€§æ’åº:")
    feature_importance = model.feature_importances_
    feature_names = FEATURES_TO_USE
    sorted_idx = np.argsort(feature_importance)[::-1]
    for i in sorted_idx:
        print(f"  {feature_names[i]}: {feature_importance[i]:.4f}")

    print(f"\n--- ç¬¬5æ­¥: ä¿å­˜CPUå…¼å®¹æ¨¡å‹åˆ° {MODEL_OUTPUT_PATH} ---")
    
    # ç¡®ä¿æ¨¡å‹å¯ä»¥åœ¨CPUä¸Šä½¿ç”¨
    if use_gpu:
        print("ğŸ”„ è½¬æ¢GPUè®­ç»ƒçš„æ¨¡å‹ä¸ºCPUå…¼å®¹ç‰ˆæœ¬...")
        # åœ¨æ–°ç‰ˆXGBoostä¸­ï¼Œç›´æ¥å°†deviceè®¾ç½®ä¸º'cpu'å³å¯
        model.set_params(device='cpu')
    
    joblib.dump(model, MODEL_OUTPUT_PATH)
    print("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸï¼(CPUå…¼å®¹)")
    
    # ä¿å­˜ç‰¹å¾åˆ—è¡¨ä»¥ä¾¿åç»­ä½¿ç”¨
    feature_info = {
        'features': FEATURES_TO_USE,
        'feature_importance': dict(zip(FEATURES_TO_USE, feature_importance)),
        'training_info': {
            'used_gpu': use_gpu,
            'scale_pos_weight': scale_pos_weight,
            'training_samples': len(X_train),
            'validation_samples': len(X_val),
            'model_params': model_params
        }
    }
    feature_info_path = os.path.join(DATA_DIR, 'feature_info.joblib')
    joblib.dump(feature_info, feature_info_path)
    print(f"ç‰¹å¾ä¿¡æ¯å·²ä¿å­˜åˆ°: {feature_info_path}")


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚"""
    print("=== XGBoosté›·è¾¾æ‚æ³¢åˆ†ç±»å™¨è®­ç»ƒ (GPUåŠ é€Ÿç‰ˆ) ===\n")
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import xgboost
        from sklearn.model_selection import train_test_split
        import joblib
        import tqdm
        import pyarrow
        print("âœ… åŸºç¡€ä¾èµ–åº“æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„åº“ -> {e}")
        print("è¯·è¿è¡Œ 'pip install pandas numpy xgboost scikit-learn joblib tqdm pyarrow' æ¥å®‰è£…ä¾èµ–ã€‚")
        return

    # å¯é€‰çš„GPUæ£€æµ‹åº“
    try:
        import GPUtil
        print("âœ… GPUæ£€æµ‹åº“å¯ç”¨")
    except ImportError:
        print("âš ï¸  GPUæ£€æµ‹åº“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€GPUæ£€æµ‹")

    dataset = prepare_dataset()
    train_model(dataset)


if __name__ == "__main__":
    main() 