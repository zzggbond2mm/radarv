#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯å¢ƒæ‚æ³¢ä¸“é¡¹å­¦ä¹ è®­ç»ƒå™¨
- ä½¿ç”¨ before_track_points.csv ä½œä¸ºç¯å¢ƒæ‚æ³¢è¿›è¡Œè®­ç»ƒ
- ä½¿ç”¨ found_points.csv å’Œ unfound_points.csv è¿›è¡ŒéªŒè¯
- æ”¯æŒæ¨¡å‹å¾®è°ƒåŠŸèƒ½
"""

import os
import sys
import pandas as pd
import numpy as np
import glob
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(PROJECT_ROOT)

# --- é…ç½® ---
DATA_DIR = os.path.dirname(os.path.abspath(__file__))
TRAIN_DATA_PATH = os.path.join(PROJECT_ROOT, 'train')
TEST_DATA_PATH = os.path.join(PROJECT_ROOT, 'test')

# ç‰¹å¾åˆ—
FEATURES_TO_USE = [
    'range_out', 
    'v_out', 
    'azim_out', 
    'elev1', 
    'energy', 
    'energy_dB', 
    'SNR/10'
]

# æ¨¡å‹ä¿å­˜è·¯å¾„
MODEL_OUTPUT_PATH = os.path.join(DATA_DIR, 'environment_clutter_model.joblib')
FEATURE_INFO_PATH = os.path.join(DATA_DIR, 'environment_clutter_feature_info.joblib')

def load_files_with_progress(path_pattern, description):
    """ä½¿ç”¨è¿›åº¦æ¡åŠ è½½æ–‡ä»¶"""
    all_files = glob.glob(path_pattern, recursive=True)
    if not all_files:
        print(f"è­¦å‘Š: åœ¨è·¯å¾„ '{path_pattern}' æœªæ‰¾åˆ°æ–‡ä»¶ã€‚")
        return pd.DataFrame()
    
    df_list = []
    print(f"å‘ç° {len(all_files)} ä¸ªæ–‡ä»¶")
    
    with tqdm(total=len(all_files), desc=description) as pbar:
        for file_path in all_files:
            try:
                df = pd.read_csv(file_path)
                if not df.empty:
                    folder_name = os.path.basename(os.path.dirname(file_path))
                    df['data_source'] = folder_name
                    df_list.append(df)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
            pbar.update(1)
            
    if not df_list:
        return pd.DataFrame()
    
    return pd.concat(df_list, ignore_index=True)

def prepare_environment_clutter_data():
    """å‡†å¤‡ç¯å¢ƒæ‚æ³¢è®­ç»ƒæ•°æ®"""
    print("=== ç¯å¢ƒæ‚æ³¢æ•°æ®å‡†å¤‡ ===")
    
    # 1. åŠ è½½ç¯å¢ƒæ‚æ³¢æ•°æ® (before_track_points.csv)
    print("1. åŠ è½½ç¯å¢ƒæ‚æ³¢æ•°æ®...")
    env_clutter_path = os.path.join(TRAIN_DATA_PATH, '**', 'before_track_points.csv')
    env_clutter_df = load_files_with_progress(env_clutter_path, "åŠ è½½ç¯å¢ƒæ‚æ³¢")
    
    if env_clutter_df.empty:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ç¯å¢ƒæ‚æ³¢æ•°æ®")
        return None, None
    
    # æ·»åŠ æ ‡ç­¾ï¼šç¯å¢ƒæ‚æ³¢æ ‡è®°ä¸º0
    env_clutter_df['label'] = 0
    print(f"ç¯å¢ƒæ‚æ³¢æ ·æœ¬æ•°: {len(env_clutter_df)}")
    
    # 2. æ£€æŸ¥å’Œæ¸…ç†ç‰¹å¾
    available_features = [f for f in FEATURES_TO_USE if f in env_clutter_df.columns]
    if len(available_features) != len(FEATURES_TO_USE):
        missing = set(FEATURES_TO_USE) - set(available_features)
        print(f"è­¦å‘Š: ç¼ºå°‘ç‰¹å¾åˆ—: {missing}")
        print(f"å°†ä½¿ç”¨å¯ç”¨ç‰¹å¾: {available_features}")
    
    # é€‰æ‹©éœ€è¦çš„åˆ—å¹¶æ¸…ç†æ•°æ®
    cols_to_keep = available_features + ['label', 'data_source']
    env_clutter_df = env_clutter_df[cols_to_keep].dropna()
    
    print(f"æ¸…ç†åç¯å¢ƒæ‚æ³¢æ ·æœ¬æ•°: {len(env_clutter_df)}")
    
    return env_clutter_df, available_features

def prepare_validation_data(use_test_data=True):
    """å‡†å¤‡éªŒè¯æ•°æ®"""
    data_path = TEST_DATA_PATH if use_test_data else TRAIN_DATA_PATH
    data_source = "æµ‹è¯•" if use_test_data else "è®­ç»ƒ"
    
    print(f"2. åŠ è½½{data_source}éªŒè¯æ•°æ®...")
    
    # åŠ è½½æ­£æ ·æœ¬ (found_points.csv)
    found_path = os.path.join(data_path, '**', 'found_points.csv')
    found_df = load_files_with_progress(found_path, f"åŠ è½½{data_source}æ­£æ ·æœ¬")
    
    # åŠ è½½è´Ÿæ ·æœ¬ (unfound_points.csv) 
    unfound_path = os.path.join(data_path, '**', 'unfound_points.csv')
    unfound_df = load_files_with_progress(unfound_path, f"åŠ è½½{data_source}è´Ÿæ ·æœ¬")
    
    if found_df.empty and unfound_df.empty:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°{data_source}éªŒè¯æ•°æ®")
        return pd.DataFrame()
    
    # æ·»åŠ æ ‡ç­¾
    if not found_df.empty:
        found_df['label'] = 1  # ä¿¡å·
    if not unfound_df.empty:
        unfound_df['label'] = 0  # æ‚æ³¢
    
    # åˆå¹¶éªŒè¯æ•°æ®
    val_df = pd.concat([found_df, unfound_df], ignore_index=True)
    
    # æ£€æŸ¥ç‰¹å¾åˆ—
    available_features = [f for f in FEATURES_TO_USE if f in val_df.columns]
    cols_to_keep = available_features + ['label', 'data_source']
    val_df = val_df[cols_to_keep].dropna()
    
    print(f"{data_source}éªŒè¯æ•°æ®: {len(val_df)} æ¡ (æ­£æ ·æœ¬: {len(found_df)}, è´Ÿæ ·æœ¬: {len(unfound_df)})")
    
    return val_df

def get_xgboost_params(use_gpu=False):
    """è·å–XGBoostå‚æ•°"""
    if use_gpu:
        print("ğŸš€ é…ç½®GPUè®­ç»ƒå‚æ•°...")
        try:
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
                'device': 'cuda',
                'tree_method': 'hist',
                'n_estimators': 500,
                'max_depth': 8,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42,
            }
            return params
        except Exception as e:
            print(f"GPUé…ç½®å¤±è´¥: {e}, å›é€€åˆ°CPU")
            return get_xgboost_params(use_gpu=False)
    else:
        print("ğŸ’» é…ç½®CPUè®­ç»ƒå‚æ•°...")
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'tree_method': 'hist',
            'n_estimators': 500,
            'max_depth': 8,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
        }
        return params

def train_environment_clutter_model():
    """è®­ç»ƒç¯å¢ƒæ‚æ³¢æ¨¡å‹"""
    print("=== ç¯å¢ƒæ‚æ³¢ä¸“é¡¹å­¦ä¹ è®­ç»ƒ ===\n")
    
    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    train_data, features = prepare_environment_clutter_data()
    if train_data is None:
        return
    
    # 2. å‡†å¤‡éªŒè¯æ•°æ®
    val_data = prepare_validation_data(use_test_data=True)
    if val_data.empty:
        print("è­¦å‘Š: æ— éªŒè¯æ•°æ®ï¼Œå°†ä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯")
        val_data = prepare_validation_data(use_test_data=False)
    
    if val_data.empty:
        print("âŒ é”™è¯¯: æ— å¯ç”¨çš„éªŒè¯æ•°æ®")
        return
    
    print("\n=== å¼€å§‹æ¨¡å‹è®­ç»ƒ ===")
    
    # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
    X_train = train_data[features]
    y_train = train_data['label']
    
    X_val = val_data[features]
    y_val = val_data['label']
    
    print(f"è®­ç»ƒé›†: {len(X_train)} æ¡")
    print(f"éªŒè¯é›†: {len(X_val)} æ¡")
    print(f"éªŒè¯é›†æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: {(y_val==1).sum()}:{(y_val==0).sum()}")
    
    # 4. æ£€æµ‹GPUå¹¶é…ç½®å‚æ•°
    try:
        import GPUtil
        gpus = GPUtil.getGPUs()
        use_gpu = len(gpus) > 0
    except:
        use_gpu = False
    
    model_params = get_xgboost_params(use_gpu=use_gpu)
    
    # 5. è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒ...")
    model = xgb.XGBClassifier(**model_params)
    
    try:
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            verbose=True
        )
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        if use_gpu:
            print("ğŸ”„ å°è¯•CPUè®­ç»ƒ...")
            model_params = get_xgboost_params(use_gpu=False)
            model = xgb.XGBClassifier(**model_params)
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)
            print("âœ… CPUè®­ç»ƒå®Œæˆï¼")
        else:
            raise e
    
    # 6. è¯„ä¼°æ¨¡å‹
    print("\n=== æ¨¡å‹è¯„ä¼° ===")
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val)[:, 1]
    
    print("åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_val, y_pred, target_names=['æ‚æ³¢', 'ä¿¡å·']))
    
    print("æ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(y_val, y_pred)
    print(cm)
    print(f"TP(çœŸä¿¡å·): {cm[1,1]}, FP(è¯¯åˆ¤): {cm[0,1]}, FN(æ¼åˆ¤): {cm[1,0]}, TN(çœŸæ‚æ³¢): {cm[0,0]}")
    
    # AUCè¯„åˆ†
    if len(np.unique(y_val)) > 1:
        auc = roc_auc_score(y_val, y_pred_proba)
        print(f"AUC: {auc:.4f}")
    
    # ç‰¹å¾é‡è¦æ€§
    print("\nç‰¹å¾é‡è¦æ€§:")
    feature_importance = model.feature_importances_
    sorted_idx = np.argsort(feature_importance)[::-1]
    for i in sorted_idx:
        print(f"  {features[i]}: {feature_importance[i]:.4f}")
    
    # 7. ä¿å­˜æ¨¡å‹
    print(f"\n=== ä¿å­˜æ¨¡å‹åˆ° {MODEL_OUTPUT_PATH} ===")
    
    # ç¡®ä¿CPUå…¼å®¹
    if use_gpu:
        model.set_params(device='cpu')
    
    joblib.dump(model, MODEL_OUTPUT_PATH)
    
    # ä¿å­˜ç‰¹å¾ä¿¡æ¯
    feature_info = {
        'features': features,
        'feature_importance': dict(zip(features, feature_importance)),
        'training_info': {
            'used_gpu': use_gpu,
            'training_samples': len(X_train),
            'validation_samples': len(X_val),
            'model_params': model_params
        }
    }
    joblib.dump(feature_info, FEATURE_INFO_PATH)
    
    print("âœ… ç¯å¢ƒæ‚æ³¢æ¨¡å‹è®­ç»ƒå’Œä¿å­˜å®Œæˆï¼")
    
    return model

def fine_tune_model(base_model_path, new_data_path, output_path):
    """æ¨¡å‹å¾®è°ƒåŠŸèƒ½"""
    print("=== æ¨¡å‹å¾®è°ƒ ===")
    print("æ­¤åŠŸèƒ½ç”¨äºåœ¨æ–°ç¯å¢ƒä¸‹å¾®è°ƒå·²è®­ç»ƒçš„æ¨¡å‹")
    print("è¾“å…¥å‚æ•°:")
    print(f"  - åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
    print(f"  - æ–°æ•°æ®è·¯å¾„: {new_data_path}")
    print(f"  - è¾“å‡ºè·¯å¾„: {output_path}")
    print("TODO: å®ç°å¾®è°ƒé€»è¾‘")

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    try:
        import xgboost
        import sklearn
        import joblib
        import tqdm
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·å®‰è£…: pip install xgboost scikit-learn joblib tqdm")
        sys.exit(1)
    
    # å¼€å§‹è®­ç»ƒ
    train_environment_clutter_model()